
加载数据...
tensor([[  101,  4205,  5472,  ...,     0,     0,     0],
        [  101,  2019,  4024,  ...,     0,     0,     0],
        [  101,  2045,  1005,  ...,     0,     0,     0],
        ...,
        [  101,  2035,  1996,  ...,     0,     0,     0],
        [  101, 11552,  2135,  ...,     0,     0,     0],
        [  101,  1037,  4121,  ...,     0,     0,     0]])
Time usage: 0:00:05
Some weights of the model checkpoint at /home/huyiwen/pretrained/bert-base-uncased-SST-2 were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
BERT_Model(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc): Linear(in_features=768, out_features=192, bias=True)
  (fc1): Linear(in_features=192, out_features=2, bias=True)
)
cuda
biLSTM(
  (Embedding): Embedding(30522, 300)
  (lstm): LSTM(300, 300, batch_first=True, bidirectional=True)
  (fc1): Linear(in_features=600, out_features=192, bias=True)
  (fc2): Linear(in_features=192, out_features=2, bias=True)
)
10,717,178 total parameters.
distill_lr 0.0002
Epoch [1/30]
Iter:      0,  Train Loss:  0.69,  Train Acc: 48.44%,  Val Loss:  0.22,  Val Acc: 49.97%,  Time: 0:00:01 *,  LR: 0.00
Iter:     50,  Train Loss:   0.7,  Train Acc: 46.88%,  Val Loss:  0.23,  Val Acc: 50.91%,  Time: 0:00:02 ,  LR: 0.00
Iter:    100,  Train Loss:  0.68,  Train Acc: 60.94%,  Val Loss:  0.22,  Val Acc: 58.32%,  Time: 0:00:03 ,  LR: 0.00
Epoch [2/30]
Iter:    150,  Train Loss:  0.66,  Train Acc: 56.25%,  Val Loss:  0.23,  Val Acc: 61.29%,  Time: 0:00:04 ,  LR: 0.00
Iter:    200,  Train Loss:  0.63,  Train Acc: 64.06%,  Val Loss:  0.23,  Val Acc: 62.82%,  Time: 0:00:05 ,  LR: 0.00
Epoch [3/30]
Iter:    250,  Train Loss:  0.61,  Train Acc: 70.31%,  Val Loss:  0.25,  Val Acc: 65.07%,  Time: 0:00:06 ,  LR: 0.00
Iter:    300,  Train Loss:  0.65,  Train Acc: 60.94%,  Val Loss:  0.25,  Val Acc: 66.67%,  Time: 0:00:07 ,  LR: 0.00
Epoch [4/30]
Iter:    350,  Train Loss:  0.54,  Train Acc: 71.88%,  Val Loss:  0.26,  Val Acc: 69.14%,  Time: 0:00:08 ,  LR: 0.00
Iter:    400,  Train Loss:  0.53,  Train Acc: 76.56%,  Val Loss:  0.29,  Val Acc: 69.58%,  Time: 0:00:10 ,  LR: 0.00
Epoch [5/30]
Iter:    450,  Train Loss:  0.49,  Train Acc: 78.12%,  Val Loss:  0.28,  Val Acc: 70.57%,  Time: 0:00:10 ,  LR: 0.00
Iter:    500,  Train Loss:  0.37,  Train Acc: 84.38%,  Val Loss:  0.29,  Val Acc: 71.83%,  Time: 0:00:11 ,  LR: 0.00
Epoch [6/30]
Iter:    550,  Train Loss:  0.42,  Train Acc: 85.94%,  Val Loss:  0.32,  Val Acc: 71.77%,  Time: 0:00:12 ,  LR: 0.00
Iter:    600,  Train Loss:  0.39,  Train Acc: 82.81%,  Val Loss:  0.32,  Val Acc: 73.86%,  Time: 0:00:13 ,  LR: 0.00
Iter:    650,  Train Loss:  0.32,  Train Acc: 84.38%,  Val Loss:  0.32,  Val Acc: 74.79%,  Time: 0:00:15 ,  LR: 0.00
Epoch [7/30]
Iter:    700,  Train Loss:   0.2,  Train Acc: 95.31%,  Val Loss:  0.35,  Val Acc: 73.97%,  Time: 0:00:16 ,  LR: 0.00
Iter:    750,  Train Loss:  0.21,  Train Acc: 93.75%,  Val Loss:  0.34,  Val Acc: 73.97%,  Time: 0:00:16 ,  LR: 0.00
Epoch [8/30]
Iter:    800,  Train Loss:  0.32,  Train Acc: 85.94%,  Val Loss:  0.35,  Val Acc: 74.63%,  Time: 0:00:17 ,  LR: 0.00
Iter:    850,  Train Loss:  0.26,  Train Acc: 90.62%,  Val Loss:  0.35,  Val Acc: 75.67%,  Time: 0:00:19 ,  LR: 0.00
Epoch [9/30]
Iter:    900,  Train Loss:  0.22,  Train Acc: 92.19%,  Val Loss:  0.37,  Val Acc: 75.73%,  Time: 0:00:20 ,  LR: 0.00
Iter:    950,  Train Loss:  0.17,  Train Acc: 95.31%,  Val Loss:  0.35,  Val Acc: 75.89%,  Time: 0:00:20 ,  LR: 0.00
Epoch [10/30]
Iter:   1000,  Train Loss:  0.17,  Train Acc: 95.31%,  Val Loss:  0.36,  Val Acc: 76.55%,  Time: 0:00:21 ,  LR: 0.00
Iter:   1050,  Train Loss:  0.23,  Train Acc: 90.62%,  Val Loss:  0.39,  Val Acc: 76.50%,  Time: 0:00:23 ,  LR: 0.00
Epoch [11/30]
Iter:   1100,  Train Loss:  0.15,  Train Acc: 95.31%,  Val Loss:  0.38,  Val Acc: 76.88%,  Time: 0:00:24 ,  LR: 0.00
Iter:   1150,  Train Loss:  0.12,  Train Acc: 96.88%,  Val Loss:  0.39,  Val Acc: 75.62%,  Time: 0:00:25 ,  LR: 0.00
Epoch [12/30]
Iter:   1200,  Train Loss: 0.074,  Train Acc: 98.44%,  Val Loss:   0.4,  Val Acc: 75.89%,  Time: 0:00:26 ,  LR: 0.00
Iter:   1250,  Train Loss: 0.049,  Train Acc: 98.44%,  Val Loss:   0.4,  Val Acc: 74.85%,  Time: 0:00:27 ,  LR: 0.00
Iter:   1300,  Train Loss: 0.045,  Train Acc: 100.00%,  Val Loss:   0.4,  Val Acc: 76.39%,  Time: 0:00:28 ,  LR: 0.00
Epoch [13/30]
Iter:   1350,  Train Loss: 0.054,  Train Acc: 96.88%,  Val Loss:  0.42,  Val Acc: 75.95%,  Time: 0:00:29 ,  LR: 0.00
Iter:   1400,  Train Loss: 0.069,  Train Acc: 96.88%,  Val Loss:  0.39,  Val Acc: 74.52%,  Time: 0:00:30 ,  LR: 0.00
Epoch [14/30]
Iter:   1450,  Train Loss:  0.05,  Train Acc: 98.44%,  Val Loss:  0.41,  Val Acc: 75.89%,  Time: 0:00:31 ,  LR: 0.00
Iter:   1500,  Train Loss: 0.048,  Train Acc: 98.44%,  Val Loss:  0.41,  Val Acc: 76.39%,  Time: 0:00:32 ,  LR: 0.00
Epoch [15/30]
Iter:   1550,  Train Loss: 0.013,  Train Acc: 100.00%,  Val Loss:  0.42,  Val Acc: 77.05%,  Time: 0:00:33 ,  LR: 0.00
Iter:   1600,  Train Loss: 0.012,  Train Acc: 100.00%,  Val Loss:  0.41,  Val Acc: 77.38%,  Time: 0:00:34 ,  LR: 0.00
Epoch [16/30]
Iter:   1650,  Train Loss:  0.02,  Train Acc: 100.00%,  Val Loss:  0.43,  Val Acc: 76.17%,  Time: 0:00:35 ,  LR: 0.00
Iter:   1700,  Train Loss: 0.014,  Train Acc: 100.00%,  Val Loss:  0.42,  Val Acc: 77.16%,  Time: 0:00:36 ,  LR: 0.00
Epoch [17/30]
Iter:   1750,  Train Loss: 0.015,  Train Acc: 100.00%,  Val Loss:  0.43,  Val Acc: 76.66%,  Time: 0:00:37 ,  LR: 0.00
Iter:   1800,  Train Loss: 0.0054,  Train Acc: 100.00%,  Val Loss:  0.41,  Val Acc: 77.43%,  Time: 0:00:39 ,  LR: 0.00
Iter:   1850,  Train Loss: 0.018,  Train Acc: 100.00%,  Val Loss:  0.43,  Val Acc: 77.05%,  Time: 0:00:40 ,  LR: 0.00
Epoch [18/30]
Iter:   1900,  Train Loss: 0.0049,  Train Acc: 100.00%,  Val Loss:  0.42,  Val Acc: 76.44%,  Time: 0:00:41 ,  LR: 0.00
Iter:   1950,  Train Loss: 0.0066,  Train Acc: 100.00%,  Val Loss:  0.44,  Val Acc: 76.77%,  Time: 0:00:42 ,  LR: 0.00
Epoch [19/30]
Iter:   2000,  Train Loss: 0.012,  Train Acc: 100.00%,  Val Loss:  0.44,  Val Acc: 76.55%,  Time: 0:00:43 ,  LR: 0.00
Iter:   2050,  Train Loss: 0.0065,  Train Acc: 100.00%,  Val Loss:  0.46,  Val Acc: 76.33%,  Time: 0:00:44 ,  LR: 0.00
Epoch [20/30]
Iter:   2100,  Train Loss: 0.0067,  Train Acc: 100.00%,  Val Loss:  0.42,  Val Acc: 76.88%,  Time: 0:00:45 ,  LR: 0.00
Iter:   2150,  Train Loss: 0.0049,  Train Acc: 100.00%,  Val Loss:  0.44,  Val Acc: 77.43%,  Time: 0:00:46 ,  LR: 0.00
Epoch [21/30]
Iter:   2200,  Train Loss: 0.002,  Train Acc: 100.00%,  Val Loss:  0.44,  Val Acc: 77.48%,  Time: 0:00:47 ,  LR: 0.00
Iter:   2250,  Train Loss: 0.0032,  Train Acc: 100.00%,  Val Loss:  0.44,  Val Acc: 77.43%,  Time: 0:00:48 ,  LR: 0.00
Epoch [22/30]
Iter:   2300,  Train Loss: 0.0014,  Train Acc: 100.00%,  Val Loss:  0.43,  Val Acc: 77.70%,  Time: 0:00:49 ,  LR: 0.00
Iter:   2350,  Train Loss: 0.0012,  Train Acc: 100.00%,  Val Loss:  0.45,  Val Acc: 77.16%,  Time: 0:00:50 ,  LR: 0.00
Epoch [23/30]
Iter:   2400,  Train Loss: 0.0007,  Train Acc: 100.00%,  Val Loss:  0.44,  Val Acc: 77.16%,  Time: 0:00:51 ,  LR: 0.00
Iter:   2450,  Train Loss: 0.0022,  Train Acc: 100.00%,  Val Loss:  0.44,  Val Acc: 77.27%,  Time: 0:00:52 ,  LR: 0.00
Iter:   2500,  Train Loss: 0.00072,  Train Acc: 100.00%,  Val Loss:  0.44,  Val Acc: 77.10%,  Time: 0:00:53 ,  LR: 0.00
Epoch [24/30]
Iter:   2550,  Train Loss: 0.00058,  Train Acc: 100.00%,  Val Loss:  0.43,  Val Acc: 77.16%,  Time: 0:00:54 ,  LR: 0.00
Iter:   2600,  Train Loss: 0.0016,  Train Acc: 100.00%,  Val Loss:  0.42,  Val Acc: 77.27%,  Time: 0:00:55 ,  LR: 0.00
Epoch [25/30]
Iter:   2650,  Train Loss: 0.00082,  Train Acc: 100.00%,  Val Loss:  0.45,  Val Acc: 77.05%,  Time: 0:00:56 ,  LR: 0.00
Iter:   2700,  Train Loss: 0.00033,  Train Acc: 100.00%,  Val Loss:  0.46,  Val Acc: 77.21%,  Time: 0:00:58 ,  LR: 0.00
Epoch [26/30]
Iter:   2750,  Train Loss: 0.00036,  Train Acc: 100.00%,  Val Loss:  0.44,  Val Acc: 77.27%,  Time: 0:00:59 ,  LR: 0.00
Iter:   2800,  Train Loss: 0.00058,  Train Acc: 100.00%,  Val Loss:  0.45,  Val Acc: 77.38%,  Time: 0:01:00 ,  LR: 0.00
Epoch [27/30]
Iter:   2850,  Train Loss: 0.00058,  Train Acc: 100.00%,  Val Loss:  0.44,  Val Acc: 76.83%,  Time: 0:01:01 ,  LR: 0.00
Iter:   2900,  Train Loss: 0.00023,  Train Acc: 100.00%,  Val Loss:  0.45,  Val Acc: 77.05%,  Time: 0:01:02 ,  LR: 0.00
Epoch [28/30]
Iter:   2950,  Train Loss: 0.00046,  Train Acc: 100.00%,  Val Loss:  0.45,  Val Acc: 77.16%,  Time: 0:01:03 ,  LR: 0.00
Iter:   3000,  Train Loss: 0.00029,  Train Acc: 100.00%,  Val Loss:  0.44,  Val Acc: 77.21%,  Time: 0:01:04 ,  LR: 0.00
Iter:   3050,  Train Loss: 0.00097,  Train Acc: 100.00%,  Val Loss:  0.44,  Val Acc: 77.21%,  Time: 0:01:05 ,  LR: 0.00
Epoch [29/30]
Iter:   3100,  Train Loss: 0.00068,  Train Acc: 100.00%,  Val Loss:  0.45,  Val Acc: 76.99%,  Time: 0:01:06 ,  LR: 0.00
Iter:   3150,  Train Loss: 0.0007,  Train Acc: 100.00%,  Val Loss:  0.44,  Val Acc: 77.38%,  Time: 0:01:07 ,  LR: 0.00
Epoch [30/30]
Iter:   3200,  Train Loss: 0.00062,  Train Acc: 100.00%,  Val Loss:  0.43,  Val Acc: 77.32%,  Time: 0:01:08 ,  LR: 0.00
Iter:   3250,  Train Loss: 0.00019,  Train Acc: 100.00%,  Val Loss:  0.44,  Val Acc: 77.21%,  Time: 0:01:09 ,  LR: 0.00
loss 0.4452946751282133 acc 0.7726523887973641