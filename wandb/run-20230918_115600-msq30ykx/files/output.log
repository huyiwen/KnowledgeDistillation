
加载数据...
[[101, 1037, 18385, 1010, 6057, 1998, 2633, 18276, 2128, 1011, 16603, 1997, 5053, 1998, 1996, 6841, 1998, 5687, 5469, 3152, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 4593, 2128, 27241, 23931, 2013, 1996, 6276, 1011, 2282, 2723, 1997, 2151, 2445, 12217, 7815, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2027, 3653, 23545, 2037, 4378, 24185, 1050, 1005, 1056, 4133, 2145, 2005, 1037, 11507, 10800, 1010, 2174, 14036, 2135, 3591, 1010, 2061, 2027, 19817, 4140, 2041, 1996, 7511, 2671, 1011, 4349, 3787, 1997, 11829, 1011, 7168, 9219, 1998, 28971, 2308, 1999, 8301, 8737, 2100, 4253, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2023, 2003, 1037, 17453, 14726, 19379, 12758, 2006, 2293, 1010, 3638, 1010, 2381, 1998, 1996, 2162, 2090, 2396, 1998, 6236, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 5655, 6262, 1005, 1055, 12075, 2571, 3762, 2323, 2031, 2042, 1996, 2022, 1011, 2035, 1011, 2203, 1011, 2035, 1997, 1996, 2715, 1011, 2436, 2019, 20936, 2063, 3152, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 3409, 7231, 4571, 4152, 1996, 4309, 2074, 2157, 1011, 1011, 6057, 1999, 1996, 2690, 1997, 6517, 1999, 1996, 2690, 1997, 17772, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1037, 5470, 2143, 2008, 2005, 1996, 4895, 5498, 10711, 3064, 3248, 2488, 2006, 2678, 2007, 1996, 2614, 2357, 2091, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 4562, 2102, 1998, 4068, 2290, 2024, 2119, 21688, 1010, 2096, 15876, 18620, 2102, 1012, 1012, 1012, 2003, 12047, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1037, 2210, 2625, 6034, 2084, 1999, 1996, 2627, 1010, 2007, 2936, 13080, 10071, 2090, 2068, 1010, 1998, 2007, 8491, 18201, 2015, 2000, 3338, 1996, 6945, 5007, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1996, 2143, 2003, 9975, 9410, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
[[101, 2053, 2929, 1010, 2053, 9805, 5705, 1010, 2025, 2172, 1997, 2505, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1037, 2175, 2497, 1997, 3298, 2140, 2061, 5305, 2135, 4086, 1010, 2130, 1996, 9461, 10390, 1997, 5405, 1005, 1055, 19351, 28405, 4487, 6916, 2229, 2097, 2128, 10649, 2009, 2039, 2066, 2743, 6895, 2094, 13675, 21382, 7987, 9307, 2063, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 18542, 1997, 2047, 2259, 2003, 2019, 14477, 18155, 23884, 4588, 6752, 1010, 3005, 2069, 7494, 4519, 2003, 2008, 2009, 4515, 2011, 11221, 2074, 2055, 2673, 2039, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2057, 2196, 2428, 2514, 2920, 2007, 1996, 2466, 1010, 2004, 2035, 1997, 2049, 4784, 3961, 2074, 2008, 1024, 10061, 4784, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2023, 2003, 2028, 1997, 14955, 6962, 3211, 1005, 1055, 2190, 3152, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2202, 2729, 1997, 2026, 4937, 4107, 1037, 27150, 2135, 2367, 14704, 1997, 4004, 5988, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 3772, 1010, 3391, 2011, 17214, 12821, 1010, 2471, 3084, 1036, 1036, 2196, 2153, 1005, 1005, 4276, 19927, 1010, 2021, 1011, 1048, 15185, 1011, 3213, 1032, 1013, 2472, 1011, 25269, 2497, 1011, 8040, 25293, 12494, 2323, 3582, 2010, 12960, 6040, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1996, 3185, 6526, 2005, 2049, 4715, 2895, 1998, 2049, 2986, 3772, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 7779, 1005, 1055, 5376, 2013, 2210, 3898, 2000, 2502, 2097, 2681, 29303, 2006, 2062, 2084, 1037, 2261, 5344, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2065, 2023, 6209, 3185, 2003, 4011, 2000, 2022, 1037, 5592, 1010, 8307, 4895, 13088, 29098, 2098, 2009, 2220, 1010, 2165, 2041, 2035, 1996, 2204, 4933, 1010, 1998, 2187, 2369, 1996, 10231, 1011, 1048, 15185, 1011, 6719, 1011, 25269, 2497, 1011, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]
[tensor([[  101,  4205,  5472,  ...,     0,     0,     0],
        [  101,  2019,  4024,  ...,     0,     0,     0],
        [  101,  2045,  1005,  ...,     0,     0,     0],
        ...,
        [  101,  2035,  1996,  ...,     0,     0,     0],
        [  101, 11552,  2135,  ...,     0,     0,     0],
        [  101,  1037,  4121,  ...,     0,     0,     0]]), tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]]), tensor([0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,
        1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,
        0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0])]
Time usage: 0:00:11
Some weights of the model checkpoint at /home/huyiwen/pretrained/bert-base-uncased-SST-2 were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
BERT_Model(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc): Linear(in_features=768, out_features=192, bias=True)
  (fc1): Linear(in_features=192, out_features=2, bias=True)
)
cuda
biLSTM(
  (Embedding): Embedding(30522, 300)
  (lstm): LSTM(300, 300, batch_first=True, bidirectional=True)
  (fc1): Linear(in_features=600, out_features=192, bias=True)
  (fc2): Linear(in_features=192, out_features=2, bias=True)
)
10,717,178 total parameters.
Epoch [1/30]
Iter:      0,  Train Loss: 1.6e+01,  Train Acc: 51.56%,  Val Loss: 1.4e+01,  Val Acc: 49.92%,  Time: 0:00:02 *,  LR: 0.09972609476841367
Iter:     50,  Train Loss: 1.7e+01,  Train Acc: 45.31%,  Val Loss: 1.4e+01,  Val Acc: 50.08%,  Time: 0:00:04 *,  LR: 0.07938926261462524
Iter:    100,  Train Loss: 1.7e+01,  Train Acc: 50.00%,  Val Loss: 1.4e+01,  Val Acc: 50.08%,  Time: 0:00:06 *,  LR: 0.02966316784620994
Epoch [2/30]
Iter:    150,  Train Loss: 1.6e+01,  Train Acc: 43.75%,  Val Loss: 1.4e+01,  Val Acc: 50.08%,  Time: 0:00:07 ,  LR: 0.0002739052315863355
Iter:    200,  Train Loss: 1.6e+01,  Train Acc: 43.75%,  Val Loss: 1.4e+01,  Val Acc: 50.58%,  Time: 0:00:09 ,  LR: 0.020610737385377154
Epoch [3/30]
Iter:    250,  Train Loss: 1.6e+01,  Train Acc: 50.00%,  Val Loss: 1.4e+01,  Val Acc: 52.00%,  Time: 0:00:10 ,  LR: 0.0703368321537913
Iter:    300,  Train Loss: 1.6e+01,  Train Acc: 37.50%,  Val Loss: 1.4e+01,  Val Acc: 50.19%,  Time: 0:00:12 ,  LR: 0.09972609476842181
Epoch [4/30]
Iter:    350,  Train Loss: 1.6e+01,  Train Acc: 48.44%,  Val Loss: 1.4e+01,  Val Acc: 50.08%,  Time: 0:00:13 ,  LR: 0.07938926261463011
Iter:    400,  Train Loss: 1.7e+01,  Train Acc: 45.31%,  Val Loss: 1.4e+01,  Val Acc: 50.08%,  Time: 0:00:15 ,  LR: 0.029663167846208528
Epoch [5/30]
Iter:    450,  Train Loss: 1.7e+01,  Train Acc: 37.50%,  Val Loss: 1.4e+01,  Val Acc: 50.08%,  Time: 0:00:17 ,  LR: 0.0002739052315863355
Iter:    500,  Train Loss: 1.7e+01,  Train Acc: 54.69%,  Val Loss: 1.4e+01,  Val Acc: 50.08%,  Time: 0:00:18 ,  LR: 0.020610737385375662
Epoch [6/30]
Iter:    550,  Train Loss: 1.7e+01,  Train Acc: 42.19%,  Val Loss: 1.4e+01,  Val Acc: 50.08%,  Time: 0:00:19 ,  LR: 0.07033683215379732
Iter:    600,  Train Loss: 1.5e+01,  Train Acc: 48.44%,  Val Loss: 1.4e+01,  Val Acc: 53.54%,  Time: 0:00:21 ,  LR: 0.09972609476842378
Iter:    650,  Train Loss: 1.6e+01,  Train Acc: 50.00%,  Val Loss: 1.4e+01,  Val Acc: 50.08%,  Time: 0:00:23 ,  LR: 0.07938926261462201
Epoch [7/30]
Iter:    700,  Train Loss: 1.7e+01,  Train Acc: 51.56%,  Val Loss: 1.4e+01,  Val Acc: 50.08%,  Time: 0:00:24 ,  LR: 0.029663167846213603
Iter:    750,  Train Loss: 1.6e+01,  Train Acc: 46.88%,  Val Loss: 1.4e+01,  Val Acc: 50.08%,  Time: 0:00:26 ,  LR: 0.0002739052315863355
Epoch [8/30]
Iter:    800,  Train Loss: 1.6e+01,  Train Acc: 40.62%,  Val Loss: 1.4e+01,  Val Acc: 50.08%,  Time: 0:00:27 ,  LR: 0.02061073738538169
Iter:    850,  Train Loss: 1.8e+01,  Train Acc: 45.31%,  Val Loss: 1.4e+01,  Val Acc: 50.08%,  Time: 0:00:29 ,  LR: 0.07033683215379839
Epoch [9/30]
Iter:    900,  Train Loss: 1.6e+01,  Train Acc: 46.88%,  Val Loss: 1.4e+01,  Val Acc: 51.24%,  Time: 0:00:30 ,  LR: 0.09972609476841374
Iter:    950,  Train Loss: 1.5e+01,  Train Acc: 57.81%,  Val Loss: 1.4e+01,  Val Acc: 50.08%,  Time: 0:00:32 ,  LR: 0.07938926261461202
Epoch [10/30]
Iter:   1000,  Train Loss: 1.6e+01,  Train Acc: 40.62%,  Val Loss: 1.4e+01,  Val Acc: 50.08%,  Time: 0:00:34 *,  LR: 0.029663167846210266
Iter:   1050,  Train Loss: 1.7e+01,  Train Acc: 50.00%,  Val Loss: 1.4e+01,  Val Acc: 50.14%,  Time: 0:00:35 ,  LR: 0.0002739052315863355
Epoch [11/30]
Iter:   1100,  Train Loss: 1.9e+01,  Train Acc: 39.06%,  Val Loss: 1.4e+01,  Val Acc: 50.08%,  Time: 0:00:37 ,  LR: 0.02061073738537954
Iter:   1150,  Train Loss: 1.6e+01,  Train Acc: 53.12%,  Val Loss: 1.4e+01,  Val Acc: 50.08%,  Time: 0:00:38 ,  LR: 0.07033683215379172
Epoch [12/30]
Iter:   1200,  Train Loss: 1.6e+01,  Train Acc: 42.19%,  Val Loss: 1.4e+01,  Val Acc: 50.08%,  Time: 0:00:40 ,  LR: 0.09972609476842985
Iter:   1250,  Train Loss: 1.6e+01,  Train Acc: 56.25%,  Val Loss: 1.4e+01,  Val Acc: 50.08%,  Time: 0:00:41 ,  LR: 0.07938926261462459
Iter:   1300,  Train Loss: 1.6e+01,  Train Acc: 43.75%,  Val Loss: 1.4e+01,  Val Acc: 50.25%,  Time: 0:00:43 ,  LR: 0.02966316784620752
Epoch [13/30]
Iter:   1350,  Train Loss: 1.6e+01,  Train Acc: 35.94%,  Val Loss: 1.4e+01,  Val Acc: 50.08%,  Time: 0:00:44 ,  LR: 0.0002739052315863355
Iter:   1400,  Train Loss: 1.5e+01,  Train Acc: 45.31%,  Val Loss: 1.4e+01,  Val Acc: 50.63%,  Time: 0:00:46 ,  LR: 0.020610737385383188
Epoch [14/30]
Iter:   1450,  Train Loss: 1.7e+01,  Train Acc: 56.25%,  Val Loss: 1.4e+01,  Val Acc: 51.02%,  Time: 0:00:48 *,  LR: 0.07033683215380346
Iter:   1500,  Train Loss: 1.7e+01,  Train Acc: 45.31%,  Val Loss: 1.4e+01,  Val Acc: 50.08%,  Time: 0:00:50 ,  LR: 0.09972609476841775
Epoch [15/30]
Iter:   1550,  Train Loss: 1.6e+01,  Train Acc: 37.50%,  Val Loss: 1.4e+01,  Val Acc: 50.08%,  Time: 0:00:51 ,  LR: 0.07938926261461793
Iter:   1600,  Train Loss: 1.6e+01,  Train Acc: 54.69%,  Val Loss: 1.4e+01,  Val Acc: 50.08%,  Time: 0:00:53 ,  LR: 0.029663167846219137
Epoch [16/30]
Iter:   1650,  Train Loss: 1.8e+01,  Train Acc: 48.44%,  Val Loss: 1.4e+01,  Val Acc: 50.08%,  Time: 0:00:54 ,  LR: 0.0002739052315863355
Iter:   1700,  Train Loss: 1.8e+01,  Train Acc: 50.00%,  Val Loss: 1.4e+01,  Val Acc: 50.41%,  Time: 0:00:55 ,  LR: 0.020610737385380863
Traceback (most recent call last):
  File "/home/huyiwen/CV/bilstm/distill.py", line 70, in <module>
    student_train(T_model, S_model, cfg, train_loader, test_loader)
  File "/home/huyiwen/CV/bilstm/student.py", line 123, in student_train
    loss.backward()
  File "/home/huyiwen/miniconda3/envs/kd/lib/python3.10/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/home/huyiwen/miniconda3/envs/kd/lib/python3.10/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt