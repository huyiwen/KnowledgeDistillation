
加载数据...
tensor([[  101,  4205,  5472,  ...,     0,     0,     0],
        [  101,  2019,  4024,  ...,     0,     0,     0],
        [  101,  2045,  1005,  ...,     0,     0,     0],
        ...,
        [  101,  2035,  1996,  ...,     0,     0,     0],
        [  101, 11552,  2135,  ...,     0,     0,     0],
        [  101,  1037,  4121,  ...,     0,     0,     0]])
Time usage: 0:00:10
Some weights of the model checkpoint at /home/huyiwen/pretrained/bert-base-uncased-SST-2 were not used when initializing BertModel: ['classifier.weight', 'classifier.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
BERT_Model(
  (bert): BertModel(
    (embeddings): BertEmbeddings(
      (word_embeddings): Embedding(30522, 768, padding_idx=0)
      (position_embeddings): Embedding(512, 768)
      (token_type_embeddings): Embedding(2, 768)
      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (encoder): BertEncoder(
      (layer): ModuleList(
        (0-11): 12 x BertLayer(
          (attention): BertAttention(
            (self): BertSelfAttention(
              (query): Linear(in_features=768, out_features=768, bias=True)
              (key): Linear(in_features=768, out_features=768, bias=True)
              (value): Linear(in_features=768, out_features=768, bias=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
            (output): BertSelfOutput(
              (dense): Linear(in_features=768, out_features=768, bias=True)
              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
              (dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (intermediate): BertIntermediate(
            (dense): Linear(in_features=768, out_features=3072, bias=True)
            (intermediate_act_fn): GELUActivation()
          )
          (output): BertOutput(
            (dense): Linear(in_features=3072, out_features=768, bias=True)
            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (pooler): BertPooler(
      (dense): Linear(in_features=768, out_features=768, bias=True)
      (activation): Tanh()
    )
  )
  (fc): Linear(in_features=768, out_features=192, bias=True)
  (fc1): Linear(in_features=192, out_features=2, bias=True)
)
cuda
biLSTM(
  (Embedding): Embedding(30522, 300)
  (lstm): LSTM(300, 300, batch_first=True, bidirectional=True)
  (fc1): Linear(in_features=600, out_features=192, bias=True)
  (fc2): Linear(in_features=192, out_features=2, bias=True)
)
10,717,178 total parameters.
Epoch [1/30]
s_logits tensor([[0.4773, 0.5227],
        [0.4840, 0.5160],
        [0.4748, 0.5252],
        [0.4847, 0.5153],
        [0.4798, 0.5202],
        [0.4893, 0.5107],
        [0.4852, 0.5148],
        [0.4929, 0.5071],
        [0.4747, 0.5253],
        [0.4861, 0.5139],
        [0.4815, 0.5185],
        [0.5015, 0.4985],
        [0.4974, 0.5026],
        [0.4883, 0.5117],
        [0.4966, 0.5034],
        [0.4848, 0.5152],
        [0.4911, 0.5089],
        [0.4943, 0.5057],
        [0.4831, 0.5169],
        [0.4812, 0.5188],
        [0.4877, 0.5123],
        [0.4851, 0.5149],
        [0.4872, 0.5128],
        [0.4819, 0.5181],
        [0.4877, 0.5123],
        [0.4821, 0.5179],
        [0.5041, 0.4959],
        [0.4794, 0.5206],
        [0.4903, 0.5097],
        [0.4899, 0.5101],
        [0.4875, 0.5125],
        [0.4926, 0.5074],
        [0.4905, 0.5095],
        [0.4951, 0.5049],
        [0.4869, 0.5131],
        [0.4880, 0.5120],
        [0.4870, 0.5130],
        [0.4922, 0.5078],
        [0.4858, 0.5142],
        [0.4865, 0.5135],
        [0.4834, 0.5166],
        [0.4867, 0.5133],
        [0.4930, 0.5070],
        [0.4832, 0.5168],
        [0.4877, 0.5123],
        [0.4843, 0.5157],
        [0.4855, 0.5145],
        [0.4865, 0.5135],
        [0.4893, 0.5107],
        [0.4866, 0.5134],
        [0.4911, 0.5089],
        [0.4856, 0.5144],
        [0.4889, 0.5111],
        [0.4826, 0.5174],
        [0.4836, 0.5164],
        [0.4897, 0.5103],
        [0.4882, 0.5118],
        [0.4888, 0.5112],
        [0.4863, 0.5137],
        [0.4895, 0.5105],
        [0.4830, 0.5170],
        [0.4989, 0.5011],
        [0.4815, 0.5185],
        [0.4785, 0.5215]], device='cuda:0', grad_fn=<SoftmaxBackward0>) label tensor([0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0,
        0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0,
        1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1], device='cuda:0')
t_logits tensor([[9.9947e-01, 5.3368e-04],
        [1.2205e-02, 9.8779e-01],
        [9.9808e-01, 1.9192e-03],
        [9.9885e-01, 1.1547e-03],
        [3.9347e-03, 9.9607e-01],
        [5.3743e-03, 9.9463e-01],
        [9.5874e-01, 4.1255e-02],
        [3.5135e-03, 9.9649e-01],
        [9.2223e-01, 7.7773e-02],
        [4.1820e-03, 9.9582e-01],
        [9.9900e-01, 9.9894e-04],
        [9.9904e-01, 9.6125e-04],
        [9.1609e-01, 8.3912e-02],
        [3.0861e-03, 9.9691e-01],
        [4.1088e-03, 9.9589e-01],
        [9.9472e-01, 5.2836e-03],
        [1.4192e-02, 9.8581e-01],
        [1.5545e-02, 9.8446e-01],
        [4.5848e-02, 9.5415e-01],
        [5.4886e-03, 9.9451e-01],
        [9.9874e-01, 1.2551e-03],
        [9.9950e-01, 5.0455e-04],
        [9.9912e-01, 8.7586e-04],
        [9.9855e-01, 1.4496e-03],
        [3.8550e-03, 9.9615e-01],
        [3.0349e-03, 9.9697e-01],
        [9.9906e-01, 9.3514e-04],
        [9.9930e-01, 7.0369e-04],
        [4.0615e-03, 9.9594e-01],
        [4.2186e-03, 9.9578e-01],
        [4.2164e-03, 9.9578e-01],
        [9.9280e-01, 7.2048e-03],
        [9.9821e-01, 1.7895e-03],
        [9.9936e-01, 6.4397e-04],
        [3.5035e-03, 9.9650e-01],
        [9.9912e-01, 8.8046e-04],
        [9.9697e-01, 3.0259e-03],
        [9.9921e-01, 7.8953e-04],
        [9.9807e-01, 1.9271e-03],
        [9.9935e-01, 6.5100e-04],
        [4.0775e-03, 9.9592e-01],
        [9.7800e-01, 2.1997e-02],
        [9.1303e-03, 9.9087e-01],
        [2.9660e-03, 9.9703e-01],
        [7.3085e-03, 9.9269e-01],
        [9.9474e-01, 5.2644e-03],
        [9.6431e-01, 3.5686e-02],
        [9.9511e-01, 4.8911e-03],
        [9.9947e-01, 5.2820e-04],
        [9.9789e-01, 2.1140e-03],
        [9.9562e-01, 4.3801e-03],
        [9.9917e-01, 8.3110e-04],
        [5.5824e-03, 9.9442e-01],
        [3.1015e-03, 9.9690e-01],
        [5.6073e-03, 9.9439e-01],
        [9.6293e-01, 3.7073e-02],
        [1.6173e-02, 9.8383e-01],
        [1.1644e-02, 9.8836e-01],
        [7.2993e-03, 9.9270e-01],
        [9.9913e-01, 8.7073e-04],
        [4.8140e-03, 9.9519e-01],
        [9.9937e-01, 6.2513e-04],
        [5.6990e-03, 9.9430e-01],
        [5.8538e-03, 9.9415e-01]], device='cuda:0')
base_loss tensor(0.6930, device='cuda:0', grad_fn=<NllLossBackward0>) distillation_loss tensor(0.2418, device='cuda:0', grad_fn=<MseLossBackward0>)
tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]])
==> name Embedding.weight torch.Size([30522, 300]) tensor([[-2.9262e-05,  3.3261e-06, -1.2510e-05,  ...,  2.6411e-05,
          9.3781e-06, -2.2252e-05],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00]], device='cuda:0')
==> name lstm.weight_ih_l0 torch.Size([1200, 300]) tensor([[-1.6675e-05, -1.2395e-04,  1.2947e-05,  ...,  2.6398e-05,
         -7.8243e-05, -3.9246e-06],
        [ 1.7894e-05,  2.1899e-05, -1.7055e-05,  ..., -1.9407e-06,
          2.9978e-05,  7.4290e-06],
        [-1.9433e-05, -4.1803e-05, -7.2696e-07,  ...,  1.3892e-05,
          1.0950e-06, -7.2214e-07],
        ...,
        [-1.1859e-05, -2.4304e-05, -4.5703e-06,  ...,  3.4159e-05,
         -4.0184e-05,  1.7461e-05],
        [ 3.7486e-05,  9.1287e-06, -3.0394e-05,  ...,  4.5073e-05,
          1.5995e-05,  2.1748e-05],
        [ 7.7779e-06,  1.6416e-05,  8.6093e-07,  ..., -8.8693e-08,
          4.1222e-06, -2.4020e-05]], device='cuda:0')
==> name lstm.weight_hh_l0 torch.Size([1200, 300]) tensor([[-3.7336e-06, -2.0494e-06,  4.5468e-06,  ..., -2.3412e-06,
          5.6244e-06,  6.9529e-06],
        [-2.5878e-06,  1.2885e-06, -5.6715e-06,  ..., -3.1671e-06,
         -3.8238e-06,  7.0945e-07],
        [ 8.8602e-07, -1.7575e-06,  3.7479e-07,  ..., -1.8463e-06,
         -8.4626e-07, -2.3475e-06],
        ...,
        [-5.9021e-06, -5.6931e-07, -4.9867e-06,  ..., -1.9351e-06,
          4.2306e-06, -1.9776e-06],
        [-2.2010e-06,  3.1025e-06,  7.8324e-08,  ...,  5.9722e-07,
          2.1278e-06,  3.9274e-06],
        [ 3.6668e-07,  3.8130e-06, -1.2545e-06,  ...,  3.4301e-06,
          4.1163e-06, -9.4094e-06]], device='cuda:0')
==> name lstm.bias_ih_l0 torch.Size([1200]) tensor([-3.3297e-05, -4.2846e-05, -1.7954e-06,  ..., -5.3846e-06,
        -1.3373e-05,  4.4864e-05], device='cuda:0')
==> name lstm.bias_hh_l0 torch.Size([1200]) tensor([-3.3297e-05, -4.2846e-05, -1.7954e-06,  ..., -5.3846e-06,
        -1.3373e-05,  4.4864e-05], device='cuda:0')
==> name lstm.weight_ih_l0_reverse torch.Size([1200, 300]) tensor([[ 2.7366e-05, -3.3096e-05,  7.1395e-06,  ...,  2.5695e-05,
          1.9459e-06, -1.1778e-05],
        [ 4.0687e-06,  7.5088e-05,  8.8975e-05,  ..., -7.3885e-05,
         -5.3952e-05, -3.6013e-05],
        [-3.8625e-06, -3.2122e-05, -3.6415e-05,  ...,  4.8187e-05,
         -3.0782e-05,  1.7536e-05],
        ...,
        [ 5.5659e-06, -2.1730e-05, -6.3470e-06,  ...,  2.4458e-05,
         -1.5605e-05, -8.4151e-06],
        [ 3.1863e-05,  8.4200e-06,  6.1444e-06,  ...,  1.9935e-05,
          1.7056e-05,  1.8474e-05],
        [ 1.8698e-05, -1.2767e-05, -2.4974e-05,  ..., -3.4304e-05,
         -5.9073e-05,  3.4120e-05]], device='cuda:0')
==> name lstm.weight_hh_l0_reverse torch.Size([1200, 300]) tensor([[-1.3184e-06,  3.5331e-06, -3.3625e-06,  ...,  2.1929e-06,
          1.5977e-06, -1.1688e-07],
        [-6.2862e-06, -5.6571e-06,  4.4191e-07,  ..., -9.4331e-06,
         -6.9160e-07,  1.1785e-06],
        [-1.2721e-06,  6.7328e-07,  1.4128e-06,  ..., -1.5895e-07,
         -3.1779e-07,  1.2732e-05],
        ...,
        [ 1.7936e-06,  1.9392e-06,  2.5891e-06,  ...,  2.0255e-06,
         -9.3464e-09, -3.8600e-06],
        [-2.9443e-06,  2.3512e-06, -2.4960e-07,  ...,  3.1394e-06,
          2.5342e-06, -3.4491e-06],
        [ 4.4323e-06, -5.3956e-06, -3.9798e-06,  ..., -1.3551e-06,
          9.7803e-06,  1.3611e-05]], device='cuda:0')
==> name lstm.bias_ih_l0_reverse torch.Size([1200]) tensor([-2.8508e-05,  1.6215e-05, -5.2606e-06,  ..., -5.0651e-06,
         1.9243e-05, -3.4533e-05], device='cuda:0')
==> name lstm.bias_hh_l0_reverse torch.Size([1200]) tensor([-2.8508e-05,  1.6215e-05, -5.2606e-06,  ..., -5.0651e-06,
         1.9243e-05, -3.4533e-05], device='cuda:0')
==> name fc1.weight torch.Size([192, 600]) tensor([[-6.1222e-04, -1.2122e-04, -2.4883e-04,  ..., -9.7706e-05,
         -1.1009e-04,  1.4417e-04],
        [-7.6041e-04, -1.8912e-04, -1.3293e-04,  ..., -7.7427e-05,
          1.9338e-04,  3.7533e-04],
        [ 2.4233e-05,  3.9308e-04, -3.0887e-04,  ...,  3.8933e-06,
         -6.0404e-05,  4.6579e-05],
        ...,
        [ 2.2092e-04,  1.7718e-04, -1.2586e-04,  ...,  1.0850e-04,
         -3.4027e-05,  1.1426e-05],
        [-7.8210e-05, -2.3422e-04, -1.1100e-04,  ...,  1.0322e-04,
          9.4698e-05,  2.3552e-04],
        [-2.9087e-04, -4.5295e-04,  2.4391e-05,  ...,  6.6775e-05,
          4.8996e-05,  5.0734e-05]], device='cuda:0')
==> name fc1.bias torch.Size([192]) tensor([ 5.6226e-04, -8.3838e-04,  2.1182e-03, -2.2384e-04, -6.7815e-04,
         2.1834e-04, -1.0497e-03,  1.5927e-04,  3.5988e-04,  1.2712e-03,
         3.1422e-03,  1.8132e-04,  7.2148e-04, -1.2059e-03, -3.1657e-04,
        -9.2604e-04,  6.8268e-04, -3.3557e-04, -9.3199e-05, -1.7811e-03,
        -7.5199e-04,  1.0350e-03,  1.6216e-03, -3.6291e-04,  5.6178e-04,
        -6.8055e-04,  1.7480e-03, -4.0759e-04,  2.9809e-04, -6.4658e-04,
        -1.3758e-04,  1.7868e-03, -2.5742e-03,  1.5758e-03, -9.4705e-04,
         5.6664e-04, -3.7312e-04,  8.2918e-04,  1.6207e-03, -4.7709e-04,
        -6.0320e-04,  6.7917e-04, -9.2833e-04,  2.0032e-04, -1.5557e-04,
         3.3028e-03, -2.9500e-04,  2.3483e-04, -6.9188e-04, -1.5995e-03,
         2.3309e-04,  3.3670e-03, -9.5246e-04,  9.9878e-04,  2.0571e-03,
         2.4971e-04, -2.5101e-03, -1.9184e-03, -3.7852e-04,  2.1922e-03,
        -1.4773e-04,  6.1039e-04, -1.7346e-04,  3.5857e-05, -5.3284e-04,
         1.7087e-03, -1.2520e-03, -9.6073e-04,  1.1754e-05, -8.6337e-04,
         1.2090e-04, -7.1399e-04, -1.0485e-03, -1.0953e-03, -1.7877e-03,
         1.5292e-03, -5.2487e-04, -9.6671e-04,  3.6639e-06, -2.8813e-04,
         2.1270e-03,  2.3279e-04, -1.4480e-04,  2.2699e-04,  2.2739e-04,
         2.1705e-04,  4.3080e-05, -5.3212e-04, -9.9161e-04, -1.8334e-04,
         1.8800e-03, -5.2364e-04, -1.5536e-03,  4.8151e-04, -1.3552e-03,
         4.1212e-04, -1.4631e-03,  2.8984e-04, -9.5717e-04, -1.2059e-04,
        -9.0790e-04, -1.1553e-03,  1.6405e-04,  8.3838e-04,  4.8298e-04,
        -7.7702e-04,  1.5311e-03,  2.2114e-03, -5.0622e-04, -8.4281e-04,
         9.9350e-05,  2.2083e-04, -2.7688e-04,  5.8869e-05,  5.6864e-04,
         8.0735e-04, -3.5500e-04, -8.0482e-04, -2.8249e-03,  2.6637e-04,
         4.7386e-04,  4.3225e-04,  3.7250e-04,  1.5984e-03,  9.0510e-05,
         2.5155e-03,  2.0997e-03, -1.6328e-04, -3.7548e-04,  1.5611e-07,
        -7.3706e-04,  3.7850e-04, -1.8924e-06, -2.4401e-04,  6.6921e-04,
        -1.0575e-03, -8.8469e-04,  1.7223e-03,  1.6766e-03,  4.1782e-04,
         7.7968e-05,  1.4380e-03, -4.0099e-04,  9.0340e-04,  9.5663e-04,
         9.0456e-04,  1.7577e-03, -2.9195e-04,  1.4660e-04,  9.6813e-04,
        -5.9849e-04, -2.3997e-04,  1.0621e-03,  1.3919e-03,  2.0983e-04,
        -1.6358e-05, -8.8070e-04, -1.0369e-03, -5.6056e-04,  1.0839e-03,
        -1.9759e-04, -1.1272e-03, -1.7065e-03, -5.6697e-04, -3.4022e-03,
        -1.1707e-04, -5.4513e-05, -2.2927e-03,  2.6006e-03, -9.4482e-04,
        -3.3527e-04,  1.3174e-04,  1.8242e-03, -2.6258e-04, -7.6088e-04,
        -7.5749e-06,  6.0920e-04, -2.2731e-03, -5.0994e-04, -1.0497e-03,
         5.7521e-06, -9.7125e-05,  1.7239e-03,  3.2278e-04,  1.9563e-04,
         6.1977e-04, -1.8846e-04,  1.6054e-03, -4.0660e-04,  4.7450e-04,
        -1.1165e-03, -2.5992e-03], device='cuda:0')
==> name fc2.weight torch.Size([2, 192]) tensor([[ 1.7879e-03, -4.1202e-04, -2.1791e-04, -7.3727e-04,  1.7946e-04,
          4.8238e-04, -2.0071e-03,  6.7500e-04, -5.3779e-04, -4.0945e-04,
         -2.6392e-03,  2.0022e-04,  2.3794e-03, -2.1262e-03, -7.5070e-04,
         -5.5976e-04, -1.3993e-03,  6.6398e-04, -2.8159e-03, -1.5120e-03,
         -4.9779e-04,  4.2740e-05,  9.9930e-04, -1.2094e-03, -2.0129e-03,
         -8.8216e-04, -1.6633e-03, -2.8212e-03,  6.3889e-04, -7.2198e-04,
          1.0110e-03, -1.2019e-04,  4.5017e-04,  2.5039e-04,  1.9362e-04,
         -4.7815e-06, -6.1127e-04, -2.5583e-03,  3.6804e-03, -1.1078e-03,
         -9.1065e-05,  8.0682e-05, -3.7936e-04, -1.0781e-03, -1.6828e-03,
         -1.1605e-03, -1.4576e-03, -8.6264e-04, -2.2924e-03, -1.1118e-03,
         -4.3894e-04, -1.2199e-03, -6.7209e-04,  2.8635e-03,  6.8026e-04,
         -1.8984e-03, -1.7025e-03, -7.6391e-04, -2.3677e-03, -1.5905e-03,
          1.6667e-03,  8.7365e-04, -6.4790e-04, -1.7849e-03, -7.8167e-04,
         -6.5100e-05, -1.7521e-03, -1.8528e-03,  2.0941e-03, -1.1530e-03,
         -7.2064e-04,  5.3780e-04, -1.0912e-03, -3.4117e-03,  9.3167e-04,
         -1.0794e-03, -3.1996e-03,  4.8885e-04, -1.3619e-03,  9.8990e-05,
          2.7476e-03, -9.3050e-06,  1.3290e-04,  2.6470e-04,  1.6701e-03,
          1.5050e-03, -1.0331e-03, -3.7589e-04, -2.1216e-03,  1.4595e-03,
          4.5562e-04, -8.7872e-05,  9.8735e-04,  9.0171e-04, -1.0505e-03,
         -1.0863e-03, -1.1471e-03,  6.2910e-05,  1.2930e-03, -2.2724e-05,
         -2.3723e-03, -2.8043e-03,  3.5108e-04, -2.2102e-03, -4.9466e-04,
          2.3494e-04, -1.2089e-03, -3.1345e-04, -3.2074e-04,  1.5090e-03,
          1.6277e-03, -1.1097e-03, -4.8228e-04, -1.4633e-05, -1.0873e-03,
         -1.0418e-03, -5.9683e-04,  4.3621e-05, -2.4141e-03,  9.2349e-04,
         -8.5254e-04, -6.2169e-04, -1.3683e-03, -1.1531e-03,  6.7771e-04,
          7.9125e-04, -9.7131e-04,  8.6021e-04, -1.7339e-03, -4.6574e-05,
         -2.7080e-03,  5.9197e-04, -1.2788e-04,  1.2279e-03, -1.2319e-03,
         -2.1349e-03, -2.7079e-03, -1.8478e-03, -1.7191e-03,  8.7790e-04,
          8.0337e-04, -3.1977e-03, -3.4166e-04,  1.8053e-03, -1.7161e-03,
         -1.3710e-03,  1.9466e-03, -2.1500e-03, -2.0574e-03, -1.8873e-03,
         -1.2536e-03, -1.4355e-03, -1.3824e-03, -3.3010e-04, -3.2285e-04,
          9.3988e-04,  9.5269e-04, -1.7340e-03,  6.0726e-04, -7.4714e-04,
          1.2354e-03, -9.8766e-04, -1.8522e-03, -1.3854e-03, -1.8945e-03,
          2.0072e-03, -2.6406e-04, -2.2931e-03,  2.0499e-03,  2.4298e-03,
          1.0827e-03,  2.1119e-04, -3.8032e-04, -1.2249e-03, -2.0009e-03,
          1.3886e-04, -1.6770e-04, -3.3605e-03,  4.7470e-04,  8.0897e-05,
         -6.8334e-04, -1.0976e-03, -3.0774e-04,  4.9955e-04, -3.1148e-03,
         -8.6201e-04, -1.4547e-03, -1.4840e-03,  1.5398e-03, -9.7668e-04,
         -4.7168e-03, -3.0252e-03],
        [-3.3862e-04, -2.4798e-03, -9.4974e-05,  3.5939e-03, -3.0549e-03,
         -9.6501e-04,  5.1605e-05, -4.8926e-04,  1.3200e-03,  2.0482e-03,
         -1.0741e-03,  1.7753e-03,  1.1571e-03,  2.1757e-03,  5.0867e-04,
          4.8363e-03,  1.2775e-03,  5.7188e-05,  4.3019e-04, -2.4323e-04,
         -7.6616e-04, -2.3951e-05,  3.1663e-03,  2.1331e-03,  1.2858e-03,
          2.8639e-03,  2.8198e-03,  7.0588e-04, -4.6493e-04, -1.0054e-03,
          2.1212e-04, -2.8470e-03,  3.2434e-03,  1.4555e-03,  1.2798e-03,
          1.8817e-03,  3.0302e-03,  3.0902e-03, -3.6872e-04,  6.2014e-04,
          1.5085e-03,  1.4414e-03,  2.8513e-03,  3.1408e-04,  1.5240e-03,
         -2.4643e-03, -1.5020e-04, -6.9982e-04,  1.7137e-03,  1.4804e-03,
         -1.7083e-04,  4.8464e-03, -6.1548e-03,  3.4118e-03, -1.9598e-03,
          9.9112e-04,  3.1902e-03,  2.7243e-03, -2.3882e-03,  5.1984e-03,
          6.4509e-04,  1.4631e-03,  1.0678e-03,  1.9861e-03, -1.7998e-03,
         -5.8933e-04,  2.3875e-03,  1.1027e-03,  5.3787e-05,  1.6540e-03,
          5.0364e-04,  1.4069e-03,  2.7224e-03, -1.0771e-03,  2.4728e-04,
          9.1989e-04,  2.4299e-03, -1.4258e-03,  4.0731e-03, -2.7414e-05,
          1.0029e-03, -1.1740e-03, -8.5796e-04,  2.3453e-04, -3.4133e-03,
         -7.3866e-04, -8.8398e-04, -2.3376e-04,  1.1347e-03,  1.0237e-03,
          2.7168e-03,  6.7578e-04,  2.7405e-03,  2.3939e-03,  1.9553e-03,
          8.7024e-04, -4.5636e-03, -1.3422e-03,  1.4116e-03,  1.6244e-03,
         -5.4993e-04, -5.4802e-05, -1.0632e-03,  1.2193e-03, -4.5191e-04,
         -3.0772e-04,  1.7902e-04,  4.5121e-03,  1.2392e-03,  4.0483e-04,
         -2.4519e-04,  6.5639e-04,  1.9619e-03,  9.0869e-04, -1.0817e-03,
          6.1588e-04,  3.3961e-03, -2.5266e-03,  7.9908e-04, -1.6668e-03,
          6.1220e-05, -1.1881e-03, -1.2198e-03,  1.5439e-03, -1.0079e-03,
          3.0711e-03,  3.1618e-03, -4.1840e-04,  2.7750e-06,  2.9047e-03,
          1.8240e-03,  1.2155e-03, -1.5911e-03, -2.3541e-03,  1.9289e-03,
          9.5083e-04,  3.2583e-03,  3.4183e-03, -1.2775e-03,  1.3142e-03,
          2.6831e-04, -6.9520e-04, -3.9194e-04,  2.7112e-03,  1.1826e-04,
          5.5439e-05,  1.5069e-03,  2.2979e-03,  1.5182e-03,  2.4701e-03,
          1.0646e-03, -5.6630e-05,  2.1681e-03,  3.6227e-03,  3.6477e-04,
          6.5558e-04,  1.1570e-03, -1.6519e-03,  1.3268e-03,  9.8197e-04,
          2.0600e-03,  1.4937e-03,  7.8636e-04,  5.3136e-04,  3.3823e-03,
         -1.6268e-03,  1.1460e-03,  1.7367e-03,  4.5674e-03,  1.9005e-03,
          1.3297e-03,  8.4268e-04,  2.3400e-03,  4.4879e-04,  1.7891e-03,
          2.5367e-04,  1.9117e-03,  5.4626e-04,  4.0413e-04,  3.0111e-04,
          1.4644e-03,  3.0637e-04,  1.5539e-03,  1.2403e-03,  1.3062e-03,
         -2.0083e-03,  3.9305e-04, -1.1966e-03,  1.0769e-03,  3.5024e-03,
          1.2179e-03,  8.3861e-04]], device='cuda:0')
==> name fc2.bias torch.Size([2]) tensor([-0.0211,  0.0211], device='cuda:0')
Iter:      0,  Train Loss:  0.24,  Train Acc: 48.44%,  Val Loss:  0.22,  Val Acc: 51.18%,  Time: 0:00:02 *,  LR: 0.9972609476841366
s_logits tensor([[0.4682, 0.5318],
        [0.4528, 0.5472],
        [0.4708, 0.5292],
        [0.4659, 0.5341],
        [0.4724, 0.5276],
        [0.4661, 0.5339],
        [0.4499, 0.5501],
        [0.4740, 0.5260],
        [0.4671, 0.5329],
        [0.4670, 0.5330],
        [0.4618, 0.5382],
        [0.4621, 0.5379],
        [0.4651, 0.5349],
        [0.4651, 0.5349],
        [0.4650, 0.5350],
        [0.4683, 0.5317],
        [0.4613, 0.5387],
        [0.4680, 0.5320],
        [0.4591, 0.5409],
        [0.4757, 0.5243],
        [0.4648, 0.5352],
        [0.4657, 0.5343],
        [0.4643, 0.5357],
        [0.4608, 0.5392],
        [0.4699, 0.5301],
        [0.4827, 0.5173],
        [0.4592, 0.5408],
        [0.4680, 0.5320],
        [0.4683, 0.5317],
        [0.4686, 0.5314],
        [0.4654, 0.5346],
        [0.4513, 0.5487],
        [0.4732, 0.5268],
        [0.4649, 0.5351],
        [0.4820, 0.5180],
        [0.4653, 0.5347],
        [0.4582, 0.5418],
        [0.4667, 0.5333],
        [0.4414, 0.5586],
        [0.4623, 0.5377],
        [0.4608, 0.5392],
        [0.4639, 0.5361],
        [0.4622, 0.5378],
        [0.4645, 0.5355],
        [0.4517, 0.5483],
        [0.4581, 0.5419],
        [0.4794, 0.5206],
        [0.4658, 0.5342],
        [0.4524, 0.5476],
        [0.4606, 0.5394],
        [0.4609, 0.5391],
        [0.4719, 0.5281],
        [0.4611, 0.5389],
        [0.4733, 0.5267],
        [0.4818, 0.5182],
        [0.4581, 0.5419],
        [0.4640, 0.5360],
        [0.4593, 0.5407],
        [0.4491, 0.5509],
        [0.4612, 0.5388],
        [0.4442, 0.5558],
        [0.4633, 0.5367],
        [0.4623, 0.5377],
        [0.4660, 0.5340]], device='cuda:0', grad_fn=<SoftmaxBackward0>) label tensor([0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,
        1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0,
        1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1], device='cuda:0')
t_logits tensor([[3.6535e-03, 9.9635e-01],
        [8.5200e-02, 9.1480e-01],
        [9.9759e-01, 2.4075e-03],
        [9.8347e-01, 1.6529e-02],
        [9.9656e-01, 3.4396e-03],
        [9.9897e-01, 1.0284e-03],
        [9.9876e-01, 1.2386e-03],
        [8.9474e-01, 1.0526e-01],
        [1.7154e-02, 9.8285e-01],
        [7.8416e-03, 9.9216e-01],
        [9.9947e-01, 5.3270e-04],
        [9.9902e-01, 9.7864e-04],
        [9.9658e-01, 3.4199e-03],
        [7.4760e-03, 9.9252e-01],
        [5.5121e-03, 9.9449e-01],
        [9.9818e-01, 1.8217e-03],
        [9.9763e-01, 2.3679e-03],
        [9.9828e-01, 1.7159e-03],
        [9.9196e-01, 8.0375e-03],
        [9.8732e-01, 1.2684e-02],
        [9.9913e-01, 8.7289e-04],
        [9.9762e-01, 2.3799e-03],
        [2.9597e-03, 9.9704e-01],
        [9.9868e-01, 1.3227e-03],
        [9.8777e-01, 1.2229e-02],
        [3.2997e-02, 9.6700e-01],
        [4.0559e-03, 9.9594e-01],
        [1.0727e-02, 9.8927e-01],
        [1.6889e-02, 9.8311e-01],
        [9.9938e-01, 6.1934e-04],
        [3.1060e-03, 9.9689e-01],
        [9.9907e-01, 9.3411e-04],
        [9.9672e-01, 3.2779e-03],
        [4.5648e-03, 9.9544e-01],
        [3.7807e-02, 9.6219e-01],
        [5.6310e-03, 9.9437e-01],
        [9.8587e-01, 1.4132e-02],
        [9.8950e-01, 1.0499e-02],
        [3.9312e-03, 9.9607e-01],
        [4.4445e-03, 9.9556e-01],
        [9.9902e-01, 9.7928e-04],
        [9.9541e-01, 4.5851e-03],
        [9.9492e-01, 5.0788e-03],
        [9.9911e-01, 8.9490e-04],
        [4.4977e-03, 9.9550e-01],
        [5.4439e-03, 9.9456e-01],
        [9.9627e-01, 3.7299e-03],
        [4.3933e-03, 9.9561e-01],
        [9.9934e-01, 6.6125e-04],
        [9.9890e-01, 1.0952e-03],
        [3.5763e-03, 9.9642e-01],
        [3.7101e-03, 9.9629e-01],
        [3.5931e-03, 9.9641e-01],
        [5.6526e-03, 9.9435e-01],
        [9.9926e-01, 7.4279e-04],
        [3.3738e-03, 9.9663e-01],
        [9.9906e-01, 9.4135e-04],
        [9.7151e-01, 2.8491e-02],
        [9.8060e-02, 9.0194e-01],
        [9.9738e-01, 2.6226e-03],
        [4.0014e-03, 9.9600e-01],
        [9.9899e-01, 1.0121e-03],
        [9.9566e-01, 4.3449e-03],
        [2.7041e-03, 9.9730e-01]], device='cuda:0')
base_loss tensor(0.6973, device='cuda:0', grad_fn=<NllLossBackward0>) distillation_loss tensor(0.2455, device='cuda:0', grad_fn=<MseLossBackward0>)
tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]])
==> name Embedding.weight torch.Size([30522, 300]) tensor([[-5.9844e-07,  9.0305e-06,  1.1644e-05,  ..., -4.1284e-06,
         -3.8461e-06,  3.3242e-06],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00]], device='cuda:0')
==> name lstm.weight_ih_l0 torch.Size([1200, 300]) tensor([[-5.0411e-05,  2.3301e-05, -8.5200e-07,  ..., -5.1328e-05,
         -2.0233e-05,  1.3085e-05],
        [-1.9180e-05,  1.2583e-05, -9.7413e-06,  ...,  1.8037e-05,
          3.3330e-05,  4.5852e-06],
        [-3.9269e-06,  8.6265e-06,  7.0550e-06,  ...,  1.4774e-06,
         -2.6895e-06, -1.5057e-07],
        ...,
        [-7.0634e-06, -2.4678e-05,  4.5584e-05,  ...,  1.4237e-05,
          1.6924e-05,  5.6989e-06],
        [ 3.1832e-05, -6.9445e-06, -4.1989e-05,  ..., -3.1437e-05,
          1.8249e-05, -9.9994e-06],
        [ 1.3401e-05, -3.6896e-05,  1.1714e-05,  ..., -3.9225e-07,
          9.3496e-06,  1.2033e-05]], device='cuda:0')
==> name lstm.weight_hh_l0 torch.Size([1200, 300]) tensor([[ 4.3073e-06, -8.0334e-06, -5.3261e-06,  ..., -8.7768e-06,
         -6.4169e-06,  3.8692e-07],
        [-6.5958e-06,  1.7024e-06, -2.0342e-06,  ..., -9.6872e-07,
         -2.4247e-06, -1.1002e-05],
        [ 7.2159e-06,  3.7779e-06,  2.3290e-07,  ..., -2.1673e-06,
         -6.7728e-07,  2.9166e-06],
        ...,
        [ 7.5933e-07,  1.3226e-07, -4.2701e-08,  ..., -8.6365e-07,
         -3.1177e-06, -1.7374e-06],
        [-4.8799e-06,  1.7436e-06,  9.8917e-06,  ...,  2.4316e-06,
         -2.7131e-07, -1.1234e-06],
        [ 8.1740e-06, -7.1898e-07,  2.9980e-06,  ...,  8.1460e-08,
         -2.5249e-06,  5.0263e-06]], device='cuda:0')
==> name lstm.bias_ih_l0 torch.Size([1200]) tensor([-4.8397e-05,  5.7957e-05, -1.6952e-05,  ..., -3.2312e-05,
        -2.5627e-05, -4.7659e-06], device='cuda:0')
==> name lstm.bias_hh_l0 torch.Size([1200]) tensor([-4.8397e-05,  5.7957e-05, -1.6952e-05,  ..., -3.2312e-05,
        -2.5627e-05, -4.7659e-06], device='cuda:0')
==> name lstm.weight_ih_l0_reverse torch.Size([1200, 300]) tensor([[ 1.9156e-05,  1.3805e-05,  1.8118e-05,  ...,  3.5599e-05,
          4.6705e-06, -6.2237e-06],
        [-1.4819e-05, -4.0515e-05, -2.2583e-05,  ...,  4.6452e-06,
         -3.8223e-05,  2.1037e-05],
        [ 1.8980e-05, -5.3133e-05, -4.5401e-05,  ..., -1.2067e-05,
          1.0249e-05, -9.8551e-06],
        ...,
        [-6.2214e-06,  4.9438e-05,  3.1369e-05,  ...,  1.9238e-05,
          2.2020e-05,  1.0998e-05],
        [ 4.7947e-05, -4.1271e-05,  6.4602e-06,  ..., -2.0174e-05,
          4.0474e-05, -3.2529e-05],
        [-1.2235e-05, -2.0386e-05, -3.8667e-05,  ..., -1.7756e-06,
         -4.7454e-06,  7.5849e-06]], device='cuda:0')
==> name lstm.weight_hh_l0_reverse torch.Size([1200, 300]) tensor([[-1.7252e-06,  2.6328e-06, -1.9190e-06,  ..., -6.1278e-06,
         -5.1106e-06,  4.0897e-06],
        [ 4.8871e-06, -7.8028e-07,  9.9835e-06,  ...,  3.1459e-06,
          1.4089e-06, -4.1636e-06],
        [ 3.8702e-06, -3.3579e-06,  2.9747e-06,  ...,  7.0436e-06,
          2.0438e-06,  7.8229e-07],
        ...,
        [-2.7031e-06,  1.0209e-06, -4.5974e-06,  ..., -9.1825e-06,
         -3.0975e-07,  7.3078e-06],
        [ 1.2719e-06,  2.9803e-06, -2.6161e-06,  ..., -4.1938e-06,
         -4.3949e-06, -4.6445e-06],
        [ 4.9553e-06, -1.2744e-06,  2.8508e-06,  ..., -8.7168e-06,
          2.0376e-06,  1.0857e-05]], device='cuda:0')
==> name lstm.bias_ih_l0_reverse torch.Size([1200]) tensor([-8.9664e-06,  2.3229e-05, -1.1544e-05,  ..., -1.0131e-05,
         1.2707e-06, -9.8852e-06], device='cuda:0')
==> name lstm.bias_hh_l0_reverse torch.Size([1200]) tensor([-8.9664e-06,  2.3229e-05, -1.1544e-05,  ..., -1.0131e-05,
         1.2707e-06, -9.8852e-06], device='cuda:0')
==> name fc1.weight torch.Size([192, 600]) tensor([[-1.7637e-04, -2.1960e-05, -1.4838e-04,  ..., -2.3947e-04,
         -1.3613e-04,  1.2474e-04],
        [-1.4653e-04,  6.1773e-05, -7.1442e-05,  ..., -1.4751e-04,
         -1.7743e-04,  1.2808e-04],
        [-2.6159e-04,  2.0810e-04,  2.5133e-05,  ..., -1.3189e-04,
         -1.1496e-05,  1.2942e-04],
        ...,
        [-9.2801e-05, -5.3980e-05, -9.4943e-05,  ..., -1.0784e-05,
         -1.0021e-04, -2.2025e-05],
        [ 1.1051e-04,  1.4896e-04, -9.9213e-06,  ...,  2.5723e-05,
          1.0733e-04, -4.9076e-05],
        [-3.2570e-05,  2.3386e-04, -2.1102e-04,  ..., -3.8629e-04,
         -1.4802e-04,  5.3023e-05]], device='cuda:0')
==> name fc1.bias torch.Size([192]) tensor([-2.5962e-03, -1.8606e-03,  1.6033e-03, -6.0247e-04,  3.6004e-04,
        -1.1938e-03, -6.2164e-04,  9.0134e-04,  1.2262e-03,  6.0923e-04,
        -4.7965e-04,  8.1605e-04, -2.5489e-04, -2.9659e-03, -8.4912e-04,
        -8.0760e-04,  1.1711e-03, -7.0765e-04, -7.3337e-04, -3.9205e-03,
        -1.3194e-03,  2.5340e-03, -1.0481e-03, -3.1792e-04, -1.9189e-03,
        -3.9422e-04,  1.6696e-03, -2.5812e-03, -1.1406e-04,  6.1580e-04,
        -1.6432e-03,  2.6709e-03,  4.4492e-04,  1.5443e-03, -2.1694e-03,
         8.5800e-04, -9.0722e-04,  9.3021e-04, -9.8546e-04, -1.2303e-03,
        -1.1130e-03,  7.3212e-04,  2.8918e-04,  3.4098e-04, -5.2351e-04,
         4.8336e-04, -1.9713e-03, -9.8929e-04,  6.7080e-04, -8.7771e-04,
         2.7536e-03,  3.2169e-03, -2.1293e-03,  3.6028e-04, -3.5937e-03,
         5.7870e-04, -1.1541e-03, -3.0499e-03,  2.8135e-05,  1.1109e-03,
         3.4062e-04,  6.2964e-04, -7.3370e-04, -1.2692e-05,  1.2260e-03,
         1.9758e-03, -8.6533e-04, -2.1514e-03, -4.5167e-04, -3.2260e-03,
         2.1334e-03,  5.0760e-04, -1.8873e-03, -6.7389e-04, -2.7798e-03,
         9.1784e-04, -1.6368e-04,  3.3346e-04,  1.0498e-03, -2.4099e-03,
        -1.4004e-03,  7.3408e-04, -1.3085e-03,  1.3720e-04,  2.7664e-03,
        -2.0796e-04, -3.7659e-05, -1.9916e-04, -1.7525e-03, -8.1732e-04,
         3.3645e-03,  1.4652e-03, -7.1947e-04,  9.1910e-04, -1.7643e-03,
         2.0339e-03,  1.7112e-03, -1.8816e-03, -3.2595e-04, -2.0578e-04,
        -1.2587e-03, -1.2098e-03,  1.8257e-04,  9.1618e-04, -1.6706e-03,
         1.5402e-03,  1.2796e-03,  1.4852e-03,  2.5565e-03,  2.9941e-04,
        -1.3465e-03,  1.2558e-03, -2.5436e-03,  1.0542e-03, -3.0130e-04,
        -1.4297e-04, -2.8716e-03, -1.0623e-04, -3.6316e-03, -2.3929e-04,
        -3.0515e-03, -3.8973e-04, -1.8353e-04,  1.7324e-03,  9.3684e-04,
         2.3579e-03,  4.7510e-03, -3.0396e-03, -1.0216e-03,  1.7639e-03,
        -1.4979e-03,  1.0355e-03, -1.5596e-03,  1.6458e-03, -1.4412e-03,
        -8.7705e-04, -1.8330e-03,  3.3196e-03,  3.5064e-03,  3.9385e-03,
         2.5286e-03,  3.5748e-03,  1.1992e-03,  9.2716e-04,  1.3385e-03,
         1.0113e-03,  4.2087e-04, -5.3910e-04,  1.2744e-03,  1.2116e-03,
        -1.6818e-03, -1.2862e-03,  3.0308e-03,  5.9888e-04,  1.0987e-03,
         5.2733e-04, -1.8535e-03, -2.3390e-03,  3.9229e-05, -7.4438e-04,
        -2.0482e-03,  2.2153e-04, -4.1680e-03, -4.2105e-04, -2.5349e-03,
         5.9658e-05,  1.7763e-03, -1.2368e-03,  8.8769e-04, -4.7654e-06,
        -6.8363e-04, -3.9106e-04,  1.5485e-03, -4.7645e-04,  3.3680e-04,
         8.6888e-04,  8.6697e-04, -1.5635e-03, -2.7754e-04,  5.3035e-04,
         8.5621e-04,  3.3205e-04,  1.3001e-03,  6.3655e-04,  1.9683e-03,
        -5.5242e-04,  1.6677e-03,  4.2408e-04,  3.9446e-04,  9.5194e-04,
        -1.1687e-03, -2.9957e-03], device='cuda:0')
==> name fc2.weight torch.Size([2, 192]) tensor([[-2.3986e-03, -1.8856e-03, -3.0385e-03, -1.4083e-03,  1.1924e-04,
         -3.1455e-03, -9.0692e-04, -8.7044e-04, -1.0826e-03, -1.8940e-03,
         -2.3901e-03, -2.1889e-03, -1.4726e-03, -2.8537e-03, -7.5612e-03,
         -1.1635e-03, -4.2051e-03, -2.0720e-03, -7.4372e-04, -4.2126e-03,
         -3.6964e-03, -1.1488e-03, -1.5368e-03, -3.3833e-03, -2.3962e-03,
         -2.1197e-03,  8.3579e-04, -4.0369e-03, -9.4963e-04, -1.9842e-03,
         -1.6560e-03, -3.0604e-03, -4.0417e-03, -1.7048e-04, -2.8472e-03,
          2.0868e-04, -3.3284e-04, -2.6671e-03, -1.5402e-03, -2.1907e-03,
         -3.6574e-04, -1.7807e-04, -2.4894e-03, -2.9924e-03, -1.4751e-03,
          1.1329e-04, -1.6807e-03, -5.8801e-04, -1.2904e-04, -2.4691e-03,
         -3.4470e-03, -2.4693e-03, -2.5081e-03,  4.3937e-04, -3.2093e-04,
         -1.0888e-03,  5.7870e-04, -1.0827e-03, -1.7375e-03, -5.1792e-04,
         -3.8411e-03, -9.8153e-04, -3.7575e-03, -8.2940e-04, -1.9474e-03,
         -9.3923e-04, -1.7677e-03, -4.7013e-03, -2.5794e-05, -2.9853e-03,
          1.6660e-04, -1.6995e-03, -1.8100e-03, -1.6526e-03, -8.2805e-04,
         -1.7767e-03, -8.5341e-04, -6.8516e-04,  3.7633e-04,  1.2318e-03,
         -2.0997e-03,  3.6178e-04, -1.1357e-03, -3.3181e-04, -5.1474e-04,
         -5.1369e-04, -2.3040e-03, -4.9161e-05, -1.8969e-03, -6.1518e-04,
          1.0699e-04, -1.9590e-03, -8.7371e-04, -1.2741e-03, -2.3857e-03,
         -3.9632e-03, -4.7443e-04, -2.8026e-03,  3.5215e-04, -7.0873e-04,
         -6.3900e-03, -4.1363e-03, -1.1476e-03, -2.2976e-03, -1.5379e-04,
         -1.4124e-03, -2.0612e-03,  5.7165e-04, -2.5456e-04, -1.6944e-03,
          1.5572e-03, -6.9549e-04,  1.1658e-03, -1.5488e-03, -8.9435e-04,
         -1.4823e-03, -4.8974e-03, -7.8299e-04, -4.3131e-03, -4.0979e-04,
         -2.2506e-03,  2.1466e-04, -4.2074e-03, -1.5690e-03, -3.9122e-03,
         -2.6978e-03, -3.5445e-03, -2.0862e-03, -8.3160e-04, -4.1882e-03,
         -3.5779e-03, -7.4993e-04, -4.3174e-03, -2.4708e-03, -2.8044e-03,
         -1.2316e-03, -3.4233e-03, -1.2773e-03, -2.7843e-03, -1.7694e-03,
         -8.6078e-05, -3.2126e-03,  1.8244e-05, -4.5034e-03, -2.8170e-03,
         -1.8488e-03, -1.3152e-03, -4.6645e-03, -1.5937e-04, -8.5502e-04,
         -2.0319e-03, -2.4337e-03,  1.3450e-03, -2.8862e-03, -2.6065e-06,
         -3.9681e-05, -2.8784e-03, -2.9599e-04,  5.1662e-04,  6.0216e-04,
         -7.0274e-04, -1.4427e-04,  1.0702e-03, -3.9293e-03, -4.9167e-03,
         -4.2605e-03, -1.6191e-03,  8.8320e-04, -3.3925e-03, -9.3194e-04,
         -1.6262e-03, -8.3842e-04, -8.8805e-04, -1.6863e-03, -6.3042e-03,
         -1.4979e-03, -2.7473e-03, -4.2999e-03, -3.6179e-03, -5.7649e-04,
         -3.0822e-03, -2.3487e-03, -3.0205e-05, -1.6891e-03,  1.9857e-03,
          1.3289e-03, -1.9345e-03, -7.7310e-07,  6.4692e-04, -4.6411e-04,
         -3.7663e-03, -5.6916e-03],
        [ 1.1103e-03,  1.7684e-03,  1.5257e-03,  1.5835e-05,  4.0205e-03,
          2.1608e-03,  5.4099e-04,  3.2757e-04,  1.4604e-03, -4.9700e-04,
          1.5977e-03,  1.1272e-03,  1.1428e-03,  1.4381e-03, -1.5331e-05,
         -2.0474e-04,  1.2009e-03,  1.4260e-03,  1.0436e-03,  3.5882e-03,
         -8.6004e-04,  9.3284e-04, -1.2210e-04,  1.8084e-03,  9.2515e-04,
          1.8603e-03,  1.3869e-03,  6.3336e-05,  1.5194e-03, -4.1494e-04,
          1.0061e-03,  2.9513e-03,  1.6425e-03,  1.9454e-03,  2.0457e-03,
          1.0487e-03,  1.1763e-03,  3.8488e-03,  1.9402e-03,  6.3826e-04,
          6.8283e-04,  1.2562e-03,  3.1655e-03,  3.2412e-03,  1.5685e-03,
          1.0219e-03,  1.0061e-03,  1.2280e-04,  6.0246e-04,  3.5134e-04,
          4.8020e-03,  7.1282e-04,  8.5677e-04, -7.0619e-04,  4.0930e-03,
          1.3878e-05,  2.1153e-03,  2.0431e-03,  2.3378e-03,  1.7088e-03,
          1.1553e-03,  1.6347e-03,  2.0900e-03,  1.9222e-04,  2.9181e-03,
          2.5775e-03,  5.1261e-04,  3.1876e-03,  6.9917e-04,  1.8360e-03,
          2.3652e-03,  7.9176e-04,  3.1061e-03,  1.7313e-03,  1.7587e-03,
          1.0762e-03,  1.3488e-03,  5.4130e-04,  6.8213e-03,  2.4036e-03,
          4.2170e-03,  2.2909e-03,  7.7772e-04,  1.6486e-03, -5.6543e-04,
          7.8623e-04,  3.4112e-04,  6.2008e-04,  3.4786e-03,  2.1340e-03,
          4.1392e-03, -4.0364e-05,  1.0397e-06,  2.4079e-03, -2.5479e-03,
          5.2265e-03,  4.4978e-03,  1.2404e-03,  2.2006e-03,  8.6338e-04,
          1.9086e-03, -5.0854e-04, -3.8347e-04,  1.6052e-03,  2.7545e-03,
          1.9738e-03,  1.0385e-03,  3.9633e-03,  2.6502e-03, -7.7422e-04,
          2.0466e-03,  1.0691e-05,  1.8594e-03, -2.9587e-04,  8.4402e-04,
          8.3844e-04,  4.3630e-03,  7.5473e-04,  1.2245e-03,  2.2381e-03,
          9.5501e-04, -6.8394e-04,  2.3659e-03,  2.6083e-03,  3.6042e-03,
          2.4228e-03,  6.6466e-03, -3.4501e-04, -1.0976e-03,  1.3984e-03,
          3.7127e-03,  2.5284e-03, -1.3336e-03,  3.7315e-03,  9.5541e-04,
          1.1247e-03,  9.4080e-04,  3.0854e-03,  6.0862e-03,  4.8385e-04,
          1.2427e-03,  4.3413e-03,  2.5282e-03,  5.1836e-03,  4.0414e-03,
          1.3848e-03,  1.3397e-03,  1.9065e-03,  3.8724e-03,  1.5208e-03,
          2.0510e-03,  1.9226e-03,  1.6034e-03,  2.0152e-03,  7.6998e-04,
          9.6069e-04,  1.8157e-03,  1.9644e-04,  5.2841e-04,  1.9583e-03,
          1.9265e-03, -6.9518e-04,  3.9099e-03,  1.4916e-03,  1.4171e-03,
          4.5194e-04,  1.3284e-03,  1.2483e-03,  2.6988e-03, -4.9759e-04,
          1.7591e-03,  5.4391e-04,  3.1815e-03,  2.1783e-05,  2.4603e-03,
          4.9730e-03,  3.5538e-03,  6.7773e-04,  1.4969e-03,  2.4363e-04,
          3.2017e-03,  2.5379e-03,  1.2058e-03, -2.7073e-04,  3.0358e-03,
         -6.8210e-07,  1.5174e-03, -1.7050e-04, -1.7123e-04,  8.5156e-04,
          2.3230e-03, -1.7158e-03]], device='cuda:0')
==> name fc2.bias torch.Size([2]) tensor([-0.0500,  0.0500], device='cuda:0')
Iter:     50,  Train Loss:  0.25,  Train Acc: 46.88%,  Val Loss:  0.22,  Val Acc: 48.38%,  Time: 0:00:04 *,  LR: 0.7938926261462523
        [7.5784e-03, 9.9242e-01],
        [9.9852e-01, 1.4781e-03],
        [7.6970e-03, 9.9230e-01],
        [6.6992e-03, 9.9330e-01],
        [9.9635e-01, 3.6517e-03],
        [5.7133e-03, 9.9429e-01],
        [9.9774e-01, 2.2599e-03],
        [9.9835e-01, 1.6478e-03],
        [9.9441e-01, 5.5863e-03],
        [9.9693e-01, 3.0675e-03],
        [9.9935e-01, 6.5311e-04],
        [7.7251e-03, 9.9227e-01],
        [1.8652e-02, 9.8135e-01],
        [1.5162e-02, 9.8484e-01],
        [9.8231e-01, 1.7693e-02],
        [9.9216e-01, 7.8383e-03],
        [3.7974e-03, 9.9620e-01],
        [9.9828e-01, 1.7155e-03],
        [9.9938e-01, 6.2141e-04],
        [5.5357e-02, 9.4464e-01],
        [4.0416e-03, 9.9596e-01],
        [9.9898e-01, 1.0150e-03],
        [9.3802e-03, 9.9062e-01],
        [2.8860e-02, 9.7114e-01],
        [7.6876e-02, 9.2312e-01],
        [4.5977e-03, 9.9540e-01],
        [9.9938e-01, 6.1742e-04],
        [7.1329e-03, 9.9287e-01],
        [9.9548e-01, 4.5206e-03],
        [3.4936e-03, 9.9651e-01],
        [9.9664e-01, 3.3613e-03],
        [3.8869e-03, 9.9611e-01]], device='cuda:0')
base_loss tensor(0.6923, device='cuda:0', grad_fn=<NllLossBackward0>) distillation_loss tensor(0.2391, device='cuda:0', grad_fn=<MseLossBackward0>)
tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]])
==> name Embedding.weight torch.Size([30522, 300]) tensor([[ 4.2895e-06, -4.3078e-06, -9.5901e-06,  ...,  3.3088e-07,
         -4.2810e-06, -3.3753e-06],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00]], device='cuda:0')
==> name lstm.weight_ih_l0 torch.Size([1200, 300]) tensor([[ 1.1134e-05, -1.5393e-05, -1.5435e-05,  ...,  3.1746e-05,
         -1.6046e-05, -4.2947e-05],
        [-2.2259e-05,  4.6141e-05,  4.6046e-05,  ..., -1.8878e-05,
          1.7006e-05, -3.6844e-05],
        [-1.4521e-05, -3.9541e-06,  1.3036e-05,  ..., -8.2692e-06,
          7.8398e-06, -5.2756e-05],
        ...,
        [-1.9210e-05,  5.8974e-06, -1.5267e-05,  ...,  3.2718e-05,
         -2.8807e-05,  2.3968e-05],
        [-1.4511e-06, -3.9305e-05, -3.7669e-07,  ..., -2.8350e-05,
         -1.2132e-05,  2.6677e-05],
        [ 2.6646e-05,  1.0596e-05, -4.9879e-05,  ..., -3.5353e-05,
         -9.5918e-06,  1.4179e-05]], device='cuda:0')
==> name lstm.weight_hh_l0 torch.Size([1200, 300]) tensor([[-3.6479e-06, -1.0940e-06,  9.5189e-07,  ..., -8.0461e-06,
          2.8154e-06, -2.2347e-07],
        [ 9.0061e-07, -4.1074e-06,  4.1575e-06,  ..., -3.6206e-06,
          4.4698e-06, -1.7914e-06],
        [ 4.0828e-06,  1.9894e-06,  1.1531e-06,  ...,  4.8708e-06,
          1.1951e-06,  4.2620e-07],
        ...,
        [ 4.4622e-06, -8.7215e-06, -1.2699e-06,  ..., -7.7960e-06,
         -3.7402e-06, -1.6552e-06],
        [ 1.2975e-06,  5.4024e-06,  5.0457e-06,  ...,  8.2990e-06,
          3.0930e-06,  1.2476e-06],
        [ 3.0339e-06, -4.4661e-06,  3.9427e-06,  ..., -4.5608e-06,
          1.2604e-06, -6.0176e-06]], device='cuda:0')
==> name lstm.bias_ih_l0 torch.Size([1200]) tensor([ 1.3994e-06,  3.1864e-06,  4.1229e-05,  ..., -7.8160e-05,
        -1.0436e-05,  2.1306e-05], device='cuda:0')
==> name lstm.bias_hh_l0 torch.Size([1200]) tensor([ 1.3994e-06,  3.1864e-06,  4.1229e-05,  ..., -7.8160e-05,
        -1.0436e-05,  2.1306e-05], device='cuda:0')
==> name lstm.weight_ih_l0_reverse torch.Size([1200, 300]) tensor([[-2.4453e-05,  6.8845e-06, -6.3057e-06,  ...,  1.8058e-05,
         -2.1923e-05, -1.4474e-05],
        [ 1.0830e-04,  5.8390e-05, -1.6178e-05,  ..., -4.7305e-05,
          8.4298e-05, -5.1377e-05],
        [-2.2711e-06,  4.7694e-06, -4.0802e-06,  ...,  2.3284e-06,
         -1.8584e-06,  1.6168e-05],
        ...,
        [ 6.7675e-05,  4.8358e-07, -2.3002e-05,  ...,  3.0601e-05,
          3.3147e-05,  7.3859e-07],
        [ 2.6208e-05,  1.7220e-05, -1.9233e-05,  ...,  2.5989e-06,
          2.2019e-05, -3.7519e-06],
        [-3.6759e-06,  1.7936e-05, -1.6935e-05,  ..., -5.5991e-06,
          2.7520e-05,  4.9674e-05]], device='cuda:0')
==> name lstm.weight_hh_l0_reverse torch.Size([1200, 300]) tensor([[ 4.7319e-07, -3.1432e-07, -2.5765e-06,  ..., -1.5755e-06,
          3.0146e-07, -3.6865e-06],
        [ 2.3800e-06, -1.3784e-06, -3.2332e-06,  ..., -6.9224e-06,
          1.8738e-06,  1.8117e-08],
        [ 1.8032e-06, -2.3583e-07,  8.8701e-07,  ...,  2.8492e-06,
         -3.0774e-07,  4.7480e-06],
        ...,
        [-1.8872e-06,  2.3539e-06, -5.7651e-06,  ..., -7.3550e-06,
         -1.8903e-06, -3.3612e-07],
        [ 1.5521e-06,  7.4449e-08,  2.0907e-06,  ..., -4.6201e-07,
         -3.5754e-07,  1.8727e-06],
        [ 2.7371e-06, -3.1097e-06,  4.3934e-06,  ...,  1.3616e-06,
          4.5727e-06,  3.9348e-06]], device='cuda:0')
==> name lstm.bias_ih_l0_reverse torch.Size([1200]) tensor([-1.0855e-06, -4.0618e-06, -1.7984e-05,  ..., -5.9909e-06,
         8.0564e-06,  1.3974e-05], device='cuda:0')
==> name lstm.bias_hh_l0_reverse torch.Size([1200]) tensor([-1.0855e-06, -4.0618e-06, -1.7984e-05,  ..., -5.9909e-06,
         8.0564e-06,  1.3974e-05], device='cuda:0')
==> name fc1.weight torch.Size([192, 600]) tensor([[ 2.0301e-04, -1.4700e-04,  5.7671e-05,  ..., -1.7737e-04,
         -1.3158e-05, -1.8568e-04],
        [-2.9163e-04,  1.4258e-05,  4.6423e-05,  ..., -1.0018e-04,
          2.3034e-04, -5.4825e-04],
        [-5.9930e-05, -1.7034e-04,  7.2142e-05,  ...,  2.8759e-05,
         -1.6556e-04, -4.3326e-05],
        ...,
        [-1.6169e-04,  2.2280e-04,  1.3857e-04,  ...,  9.0090e-06,
         -1.5415e-05,  1.8953e-04],
        [-1.8912e-05, -9.9823e-05,  7.7406e-05,  ..., -1.8455e-04,
          1.8721e-04,  4.1969e-05],
        [ 4.9470e-05, -9.9140e-05,  2.6651e-04,  ..., -1.2074e-04,
          3.2852e-04, -2.7626e-04]], device='cuda:0')
==> name fc1.bias torch.Size([192]) tensor([ 2.6804e-04, -1.3696e-03, -1.8937e-03, -1.0212e-03,  1.1473e-03,
        -1.2048e-03, -6.0359e-05, -1.1154e-03, -8.7559e-04,  2.6016e-04,
        -1.6834e-03, -3.5295e-04,  9.6643e-04, -9.4344e-04,  1.2715e-03,
        -7.2517e-05,  2.8715e-04,  9.3476e-05,  9.8068e-05, -2.6266e-04,
         1.5458e-03,  1.1444e-03,  3.0824e-04,  1.4749e-05,  9.7378e-04,
         1.0355e-03, -4.0402e-05,  6.8828e-05, -8.2679e-04,  4.5618e-05,
         9.3944e-04,  1.3675e-04,  1.2535e-03,  5.5311e-04,  1.3014e-03,
         7.5817e-04, -1.9041e-04, -2.7130e-05,  4.7845e-04, -6.6082e-04,
        -2.6660e-04,  4.2534e-04, -4.6335e-04, -2.0125e-04, -3.4367e-05,
        -3.0162e-05, -7.8862e-04, -5.7853e-05,  1.2130e-03, -6.2471e-04,
         4.8944e-04,  1.4308e-03,  3.5488e-04, -3.4202e-04,  2.3002e-03,
         4.7772e-04,  9.8553e-04,  6.6594e-04,  7.7836e-05,  2.9573e-04,
         4.1550e-05,  4.0024e-04, -8.4700e-04, -2.2348e-04,  6.8938e-04,
        -9.3474e-04, -3.6607e-04,  4.7452e-04,  9.8718e-05,  3.2422e-04,
        -1.3678e-03, -8.4741e-04,  6.1592e-04, -7.0493e-04,  1.9678e-03,
        -9.5356e-04,  1.0771e-04,  1.4639e-04, -6.8016e-05, -8.1459e-04,
         9.2247e-04, -8.2435e-04,  9.0646e-04,  6.2285e-04, -9.0610e-04,
         3.5809e-04, -2.9856e-04,  7.5814e-04,  4.1425e-06,  5.2673e-04,
         1.0024e-03,  5.6583e-04,  2.9014e-05,  6.1874e-04,  2.8658e-03,
        -5.1512e-04,  6.8806e-04,  1.5276e-03,  1.2757e-03,  4.3942e-04,
         2.4444e-04, -3.8341e-04, -4.0358e-04, -3.9504e-05, -3.7649e-04,
        -7.5881e-04, -3.7871e-04, -1.2840e-04, -7.9477e-04, -1.0716e-03,
        -1.5033e-05,  2.8605e-03, -6.5885e-04, -6.1931e-05,  1.1431e-04,
         1.6431e-04, -3.9089e-04,  1.7436e-03,  2.4852e-03,  7.1867e-05,
        -2.2096e-04, -1.1486e-04, -1.9589e-04,  3.2916e-04, -1.2428e-03,
        -9.3186e-04,  6.3893e-04,  4.4526e-04, -1.8735e-04, -6.3833e-04,
        -2.6428e-04,  4.1223e-04, -7.6234e-05, -4.1236e-05,  1.7956e-03,
        -1.1166e-04,  1.5210e-03, -1.1523e-03, -1.1020e-03, -1.7096e-03,
        -1.5896e-03,  4.3862e-04,  1.5678e-04, -1.2256e-03, -1.3399e-03,
         1.4485e-03, -6.8624e-04, -4.6679e-04,  4.2027e-04, -1.1719e-03,
        -8.9521e-04, -7.7877e-04, -2.7011e-04,  5.7420e-05, -5.8158e-04,
         2.7828e-04,  9.3046e-05, -7.1943e-04, -2.8942e-04,  1.3292e-06,
         1.9218e-03, -5.0853e-04,  2.1496e-03,  2.0133e-04, -2.7017e-04,
         2.9994e-04,  4.0682e-04, -9.8199e-04,  6.6435e-04,  5.1220e-04,
        -2.8518e-04, -1.3830e-03,  8.3347e-04,  1.8714e-05,  8.9544e-04,
         1.1211e-04,  4.8084e-04, -1.2791e-03,  1.1755e-03,  1.8717e-04,
         5.8020e-04, -3.2698e-05, -1.5131e-03, -5.4591e-04,  1.6628e-03,
        -1.3104e-04, -2.7888e-04,  8.6090e-05, -4.4568e-05,  8.7775e-04,
        -5.1841e-04, -2.2506e-04], device='cuda:0')
==> name fc2.weight torch.Size([2, 192]) tensor([[ 4.1003e-04, -1.7851e-03, -2.5668e-04, -1.7393e-03, -2.5270e-03,
          8.4097e-04,  9.2676e-04, -4.8626e-04,  8.7177e-04, -5.2457e-04,
          2.3321e-03, -6.8572e-04,  7.7860e-04, -6.6594e-04, -3.9696e-04,
          1.8910e-03, -6.0246e-05,  3.0891e-04,  4.1008e-04,  4.1254e-03,
          3.7830e-04, -4.1347e-04,  3.6234e-04, -3.7417e-05, -1.7689e-03,
          7.8294e-04,  4.5529e-04, -1.2183e-03, -1.0980e-03,  2.1514e-04,
          9.6498e-04, -5.5540e-04, -2.0414e-03, -6.3912e-04,  1.9610e-03,
          9.3375e-04,  4.2196e-04, -1.4383e-03,  4.4170e-04, -1.1074e-03,
          7.6809e-04,  5.7811e-04, -2.5802e-04, -3.7789e-03,  2.7816e-03,
          4.7136e-04, -2.3133e-04, -2.3570e-03, -3.9887e-04,  3.1424e-04,
          1.0718e-03, -3.3904e-03,  1.6114e-03,  1.6755e-04,  1.2222e-03,
         -2.8671e-03, -1.6763e-03,  4.0214e-03,  1.4907e-04, -6.1909e-04,
         -1.3492e-03, -2.8216e-04, -1.7529e-03,  6.2822e-04,  2.1615e-03,
         -8.2277e-04, -8.2618e-04, -6.6277e-04,  3.8007e-05,  1.0101e-03,
          1.1766e-03,  3.5281e-03,  3.7938e-03,  9.3148e-04,  1.9434e-03,
          2.2254e-03,  2.0148e-03, -1.7278e-03,  1.2287e-03,  4.1496e-04,
         -1.9630e-04, -1.2134e-03,  1.3549e-03,  5.1591e-04, -2.7421e-03,
          1.3164e-03, -3.6468e-04,  4.6083e-04,  1.6047e-04, -1.9064e-03,
         -1.7411e-03, -5.0042e-04,  2.4022e-03, -3.2909e-03,  3.1611e-03,
          1.6152e-03,  2.4979e-03,  2.2324e-03, -6.4844e-05, -9.0581e-04,
          2.0702e-03, -1.2871e-03,  8.9530e-05, -1.1407e-03, -2.9652e-04,
          7.5306e-05, -1.6306e-04, -1.5368e-03,  1.3996e-03,  3.4659e-04,
         -1.2501e-03, -1.6907e-03,  1.8263e-03,  5.7125e-04,  1.1062e-03,
         -1.7622e-03,  3.7122e-03, -2.5990e-04,  3.5797e-03, -2.7503e-03,
          4.3327e-04, -8.6467e-04, -1.1315e-04,  4.4025e-04,  2.2228e-03,
          9.1643e-04,  5.8658e-04,  1.1348e-03, -3.2655e-04,  1.1931e-03,
         -1.0197e-03, -6.8272e-04,  1.4254e-03, -1.5122e-03, -2.0717e-03,
          1.8554e-03,  8.9320e-04, -8.6805e-04,  1.7449e-03,  4.3289e-04,
          2.3432e-03,  7.1709e-04,  2.4148e-04, -5.0016e-03,  1.3420e-03,
         -1.7999e-03, -1.1919e-04,  1.5809e-03, -3.9597e-04, -7.1645e-05,
         -5.6565e-03, -3.1464e-04,  1.1180e-03, -1.3625e-03, -2.4241e-04,
         -9.6017e-04, -6.4064e-04, -1.3625e-05, -2.1361e-03, -4.4721e-04,
         -2.2681e-04, -9.1340e-04,  8.7661e-04,  2.5376e-04, -3.7342e-03,
          8.0380e-04, -5.4483e-06, -3.8425e-03,  1.4955e-03,  5.2409e-04,
          1.7870e-04,  7.6894e-04, -9.3968e-04,  1.5525e-03, -1.0677e-03,
         -2.5287e-03,  6.9710e-04, -4.5209e-03,  1.0202e-03, -1.2248e-03,
          1.6260e-03,  1.3887e-03,  9.7097e-04,  2.9589e-04,  3.8819e-03,
         -1.9278e-04, -3.3081e-03,  1.2031e-03,  2.7545e-04, -2.7551e-03,
          2.2443e-04, -7.5297e-04],
        [ 2.8649e-04,  7.7246e-04, -3.4579e-03,  3.0258e-03, -1.2507e-03,
          1.7129e-03,  3.4222e-04, -6.3063e-04,  4.7038e-04, -2.0493e-03,
          2.0569e-03, -1.7213e-04,  1.0761e-03, -5.3754e-04, -1.7272e-05,
          1.0526e-03,  1.5592e-03,  1.7258e-05,  3.1615e-04, -1.3218e-03,
          8.3656e-04,  1.3009e-03,  5.0671e-04, -2.9529e-04, -2.6306e-03,
         -1.0592e-03, -1.0146e-03,  5.7078e-04,  3.9649e-04,  4.7447e-04,
         -1.3812e-03, -2.5425e-03,  3.9208e-04, -4.6355e-04, -1.1757e-03,
         -4.3254e-04, -1.3659e-05,  2.2303e-03,  2.8990e-04,  1.5059e-03,
         -5.5366e-04,  5.7808e-04,  3.3523e-04, -8.1001e-04, -1.6587e-05,
         -1.6545e-03,  2.2868e-04,  1.2803e-04,  1.9911e-03,  1.1970e-03,
          1.7120e-03,  7.8303e-04, -1.0852e-03,  6.3524e-06, -9.9510e-04,
         -9.4758e-04, -1.5923e-04, -7.7155e-04,  7.2328e-04, -3.4690e-04,
          1.5532e-04,  2.3480e-03,  2.3871e-03, -2.9310e-03,  1.6608e-04,
         -8.9273e-04,  1.3815e-03,  4.1642e-04,  1.4032e-04, -5.5690e-04,
          8.2048e-04, -1.4121e-03, -2.0060e-03, -2.6231e-03, -9.1187e-04,
          5.1288e-04, -6.5559e-04,  2.4018e-03, -3.5761e-05,  3.5662e-04,
         -4.4033e-03, -1.9403e-03, -5.2401e-04,  5.0021e-04, -1.4171e-03,
         -9.4423e-04, -2.5480e-03, -3.4626e-03, -9.5773e-04,  1.9099e-04,
          8.7203e-04, -1.1180e-03,  5.7646e-04,  7.1518e-04,  4.9688e-04,
         -2.7495e-04,  9.5931e-04, -9.2760e-04,  1.3519e-04, -1.1769e-03,
          2.1669e-03,  6.2149e-04, -7.1897e-04,  2.5060e-04,  1.4182e-03,
         -9.9158e-04,  1.8742e-03,  4.4056e-04, -6.7540e-04,  2.1701e-03,
         -2.4742e-04,  6.6889e-04, -4.2388e-04,  1.8827e-04, -1.0478e-03,
         -7.5006e-05, -1.2526e-03,  2.4819e-03,  8.9028e-04, -1.5679e-03,
         -5.7028e-04, -6.7103e-04, -1.8542e-04,  7.8099e-04, -6.8666e-04,
         -1.6705e-03,  2.6385e-03, -2.3459e-04,  1.1214e-03,  2.9052e-03,
          1.4230e-04, -3.6303e-04,  1.2709e-03, -1.5030e-03,  1.4037e-03,
         -1.4349e-03, -2.6687e-03,  1.6775e-03,  2.0570e-04, -1.5173e-03,
         -4.6811e-04, -3.1389e-04, -2.7954e-04, -8.7084e-04,  4.0897e-04,
          3.5389e-04, -8.0853e-04, -3.5066e-04, -5.1361e-04, -1.8245e-03,
          5.6759e-04,  1.8659e-03, -5.2503e-04, -6.7869e-05,  8.3434e-04,
          4.3464e-04, -5.2097e-04,  1.2080e-03,  2.7104e-04, -7.7555e-04,
         -3.0035e-03,  1.2778e-03,  5.6890e-04,  1.8033e-03, -2.0053e-04,
          1.4440e-03, -3.5548e-04,  1.6366e-05,  1.0149e-03, -8.0819e-04,
          9.8538e-04,  2.8083e-04,  1.1123e-03, -2.9368e-04,  1.0220e-06,
         -1.7655e-05, -5.6363e-04, -1.0842e-04, -1.4002e-03,  1.4698e-03,
         -6.0496e-04,  1.5043e-03, -6.0121e-04, -5.5235e-04,  1.3940e-03,
          1.3940e-03, -4.8387e-04, -1.9310e-03, -1.3251e-04, -8.3994e-04,
          1.1910e-03, -5.2275e-04]], device='cuda:0')
==> name fc2.bias torch.Size([2]) tensor([-0.0007,  0.0007], device='cuda:0')
Iter:    100,  Train Loss:  0.24,  Train Acc: 56.25%,  Val Loss:  0.22,  Val Acc: 49.92%,  Time: 0:00:06 *,  LR: 0.2966316784620994
Epoch [2/30]
s_logits
        [1.5271e-02, 9.8473e-01],
        [9.9924e-01, 7.5767e-04],
        [6.5296e-03, 9.9347e-01],
        [9.9838e-01, 1.6195e-03],
        [4.2043e-03, 9.9580e-01],
        [9.9944e-01, 5.6369e-04],
        [6.0282e-03, 9.9397e-01],
        [9.9830e-01, 1.6955e-03],
        [9.9953e-01, 4.7178e-04],
        [3.1116e-02, 9.6888e-01],
        [9.9932e-01, 6.7784e-04],
        [9.9927e-01, 7.3154e-04],
        [3.5414e-02, 9.6459e-01],
        [9.9934e-01, 6.5629e-04],
        [9.9645e-01, 3.5453e-03],
        [6.2262e-03, 9.9377e-01],
        [4.8790e-03, 9.9512e-01],
        [5.6165e-03, 9.9438e-01],
        [5.0370e-03, 9.9496e-01],
        [5.3949e-03, 9.9461e-01],
        [3.6996e-03, 9.9630e-01],
        [9.9669e-01, 3.3065e-03],
        [9.9896e-01, 1.0432e-03],
        [9.9936e-01, 6.4149e-04],
        [2.8940e-01, 7.1060e-01],
        [3.3104e-03, 9.9669e-01],
        [9.9692e-01, 3.0811e-03],
        [9.9936e-01, 6.3563e-04],
        [9.9824e-01, 1.7643e-03],
        [2.5674e-02, 9.7433e-01],
        [9.9875e-01, 1.2533e-03],
        [8.3294e-03, 9.9167e-01],
        [9.9885e-01, 1.1472e-03],
        [9.9931e-01, 6.9159e-04]], device='cuda:0')
base_loss tensor(0.6965, device='cuda:0', grad_fn=<NllLossBackward0>) distillation_loss tensor(0.2361, device='cuda:0', grad_fn=<MseLossBackward0>)
tensor([[1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0],
        [1, 1, 1,  ..., 0, 0, 0]])
==> name Embedding.weight torch.Size([30522, 300]) tensor([[ 5.1109e-07, -2.9937e-06, -3.4041e-06,  ..., -5.6525e-06,
          2.3740e-06,  2.1891e-06],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        ...,
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00],
        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,
          0.0000e+00,  0.0000e+00]], device='cuda:0')
==> name lstm.weight_ih_l0 torch.Size([1200, 300]) tensor([[ 2.9538e-05, -8.7266e-06,  1.0440e-05,  ...,  2.6004e-05,
         -1.9205e-05, -7.6822e-06],
        [-1.3901e-06,  2.6676e-05,  1.1234e-05,  ..., -6.6206e-06,
         -5.1461e-06,  4.0665e-05],
        [ 1.0873e-05,  1.1484e-05, -5.7442e-06,  ...,  2.9802e-06,
          2.2464e-05, -1.0287e-05],
        ...,
        [-3.3658e-06,  2.2897e-05, -1.9639e-05,  ..., -2.5442e-05,
         -1.9817e-05,  2.8603e-05],
        [ 2.2341e-05, -4.7929e-06, -1.6094e-06,  ...,  7.1778e-07,
          2.8469e-06, -2.3499e-06],
        [-3.2211e-05,  2.2418e-05, -5.1831e-05,  ...,  4.3869e-06,
          3.4797e-06, -8.3368e-06]], device='cuda:0')
==> name lstm.weight_hh_l0 torch.Size([1200, 300]) tensor([[-3.2501e-06, -3.1772e-07, -1.6246e-06,  ...,  2.0681e-07,
         -2.7705e-06,  1.7819e-06],
        [ 6.4633e-06,  2.1501e-06,  4.6445e-07,  ...,  6.2749e-06,
          1.2831e-06, -2.8199e-06],
        [-1.5500e-06,  2.6979e-06, -5.3569e-06,  ...,  2.2027e-06,
         -1.1295e-06, -2.1183e-06],
        ...,
        [ 4.8234e-07,  3.0707e-07,  1.5708e-06,  ...,  6.8485e-06,
          4.8836e-07, -6.5982e-08],
        [-3.9179e-07, -2.5526e-06,  2.2946e-06,  ..., -1.7930e-06,
         -3.7656e-06,  5.8275e-06],
        [ 1.1941e-05, -8.1941e-07,  4.1530e-06,  ...,  6.6603e-06,
          1.4906e-06, -3.6335e-06]], device='cuda:0')
==> name lstm.bias_ih_l0 torch.Size([1200]) tensor([-1.1080e-05,  2.9206e-05, -2.4648e-06,  ...,  4.2950e-06,
        -1.7629e-05, -7.7947e-06], device='cuda:0')
==> name lstm.bias_hh_l0 torch.Size([1200]) tensor([-1.1080e-05,  2.9206e-05, -2.4648e-06,  ...,  4.2950e-06,
        -1.7629e-05, -7.7947e-06], device='cuda:0')
==> name lstm.weight_ih_l0_reverse torch.Size([1200, 300]) tensor([[ 2.8607e-06,  2.4475e-06,  2.5438e-06,  ..., -2.3203e-07,
         -4.1370e-05,  1.2016e-05],
        [-2.7342e-05, -1.6908e-05,  1.8750e-05,  ...,  2.1878e-06,
          1.0363e-05, -1.3360e-05],
        [ 7.6816e-06, -5.1974e-06,  9.9380e-06,  ..., -4.8184e-05,
         -8.2954e-06,  4.3030e-05],
        ...,
        [-1.5780e-05,  7.0695e-07,  3.6964e-06,  ..., -2.5147e-05,
         -1.8339e-05,  3.3414e-05],
        [-1.0596e-05, -2.2335e-05, -8.0398e-06,  ..., -9.1166e-06,
         -2.7406e-06,  1.5748e-05],
        [-2.4265e-05, -3.9579e-05, -8.9222e-06,  ...,  4.3291e-05,
         -2.8184e-05,  2.0223e-05]], device='cuda:0')
==> name lstm.weight_hh_l0_reverse torch.Size([1200, 300]) tensor([[-1.2966e-06,  1.3081e-06, -6.1040e-07,  ..., -2.2480e-06,
          1.1648e-06,  4.7854e-06],
        [-5.8339e-06,  1.0056e-05,  5.0462e-06,  ..., -6.5959e-06,
          6.9281e-07, -3.3091e-06],
        [ 4.2182e-06,  1.1793e-05, -1.7343e-06,  ...,  9.1624e-06,
          5.3881e-06, -9.7731e-06],
        ...,
        [ 8.9112e-07,  7.5139e-09, -5.2177e-06,  ..., -2.2168e-06,
          5.6978e-07,  1.5090e-06],
        [ 5.9920e-10, -1.6483e-06, -6.9056e-07,  ..., -4.2409e-08,
          2.9971e-06, -2.6406e-06],
        [-2.2591e-06, -4.6779e-06, -1.6591e-06,  ..., -1.2312e-06,
         -5.0668e-07, -1.6799e-06]], device='cuda:0')
==> name lstm.bias_ih_l0_reverse torch.Size([1200]) tensor([-4.8907e-05,  3.9894e-05,  4.1635e-05,  ...,  7.6328e-06,
         3.8950e-05,  2.2690e-06], device='cuda:0')
==> name lstm.bias_hh_l0_reverse torch.Size([1200]) tensor([-4.8907e-05,  3.9894e-05,  4.1635e-05,  ...,  7.6328e-06,
         3.8950e-05,  2.2690e-06], device='cuda:0')
==> name fc1.weight torch.Size([192, 600]) tensor([[ 1.5169e-04,  6.4668e-05,  1.1504e-04,  ...,  1.3239e-06,
          2.4247e-04,  5.4602e-05],
        [ 2.7282e-04,  2.4179e-04,  2.7967e-04,  ..., -3.3545e-05,
          3.9847e-04, -2.6847e-04],
        [ 2.2833e-04,  7.0747e-05,  4.1297e-06,  ...,  1.1837e-05,
         -1.5070e-04, -1.6023e-04],
        ...,
        [-9.7638e-06, -1.8980e-04, -1.6549e-04,  ...,  9.4843e-05,
         -2.5317e-04,  8.8996e-05],
        [ 4.7193e-05,  6.1357e-05,  4.2013e-05,  ..., -4.6359e-05,
          1.5686e-04, -1.2856e-04],
        [ 1.1632e-04,  7.0101e-05,  1.6734e-04,  ..., -1.0320e-04,
          1.9556e-04, -1.1638e-04]], device='cuda:0')
==> name fc1.bias torch.Size([192]) tensor([ 5.1887e-04, -1.6174e-03, -3.4017e-04, -2.0641e-04,  9.1961e-04,
        -4.5350e-04,  9.0073e-05,  6.7562e-04,  2.0444e-04, -1.4637e-04,
        -4.3915e-04, -4.0543e-04,  2.9290e-04, -7.5391e-04, -7.3623e-04,
        -5.6930e-05, -2.0117e-04,  1.9428e-04,  1.0372e-04, -9.1228e-04,
         1.1664e-03, -8.7790e-04,  1.5873e-03,  6.7994e-05, -2.5656e-04,
         3.8412e-04,  9.0193e-04, -3.5171e-04, -4.5155e-04,  1.4839e-04,
        -4.7972e-04,  1.4828e-03, -4.5939e-04, -4.4711e-04,  3.4465e-04,
         3.1654e-04,  3.9458e-04,  3.0122e-04, -7.2930e-04, -1.4711e-03,
        -7.7028e-04, -1.6808e-04, -4.4888e-04, -3.6342e-04, -2.4151e-04,
        -1.9953e-03, -3.8886e-04, -8.0793e-04, -5.3671e-04, -1.2498e-03,
         2.0620e-04,  9.3359e-04,  7.1391e-04,  1.0382e-03,  1.9201e-03,
         1.9664e-05, -2.9577e-04,  1.8796e-04, -4.0428e-05, -9.2016e-04,
        -8.4793e-05, -6.1275e-04,  6.8689e-05, -4.0254e-04, -1.1724e-03,
         1.5658e-04, -1.0473e-04,  9.1445e-04,  2.3563e-04, -5.2315e-04,
         2.4739e-04, -4.6754e-05,  7.5116e-04, -4.3469e-04, -3.9164e-05,
        -1.8303e-04,  1.1419e-04,  8.9662e-04,  8.6101e-05, -2.1519e-03,
        -4.7693e-05,  2.1678e-04, -8.3278e-05,  6.2502e-04,  1.0982e-03,
        -1.5314e-04,  1.8781e-04, -6.5456e-04, -9.7057e-04,  2.9108e-04,
         2.2090e-04,  1.0273e-03,  3.4503e-04,  7.5603e-04,  3.1562e-04,
         1.3966e-04,  1.3428e-03, -2.6842e-04,  1.4149e-04,  1.3244e-05,
         1.7653e-04, -2.0334e-05,  4.3464e-04, -1.1228e-03, -3.5580e-04,
        -4.2012e-05,  1.8538e-03,  5.7607e-04, -6.5207e-04,  1.2464e-03,
        -4.1243e-04, -2.2254e-03, -1.1031e-04, -1.3018e-04, -3.7074e-04,
        -6.2116e-04,  1.7040e-05,  1.3872e-03,  5.6748e-04, -5.9945e-05,
        -3.2681e-03, -2.6099e-04,  3.4378e-04,  5.1178e-04,  6.6595e-07,
         1.5403e-03, -1.1606e-04, -1.7805e-04, -1.5491e-03, -4.1470e-04,
        -1.4259e-04, -4.4032e-04, -2.1772e-04,  2.7569e-05,  7.3928e-05,
         9.7940e-04, -1.1430e-03, -7.3060e-04, -1.8269e-04, -3.4337e-04,
         1.4696e-03, -4.1401e-04,  8.8610e-04, -4.1951e-06, -1.6007e-03,
         3.2013e-04, -3.9893e-04, -1.6137e-05, -1.1376e-03,  3.3208e-04,
        -1.2161e-04, -3.4085e-04, -1.8862e-04,  6.0508e-04,  2.6785e-04,
         2.1201e-04,  5.8186e-04,  1.2385e-03,  9.6222e-04,  8.5699e-04,
        -2.0041e-03, -2.0064e-04,  1.0301e-04, -5.5670e-04, -8.4769e-04,
        -2.5948e-04, -7.0369e-05, -1.1179e-03,  1.4722e-04,  1.7828e-03,
         6.2924e-04,  4.1933e-04,  6.3417e-04,  1.9661e-04, -8.5799e-04,
         2.9117e-05,  2.4820e-04,  2.6940e-04,  2.6273e-04,  7.1407e-04,
        -9.5053e-04,  6.5576e-04,  9.7982e-04, -4.4192e-04,  2.6801e-04,
         2.0619e-04,  3.3815e-04, -3.0583e-04,  2.9732e-04, -5.7073e-04,
        -2.7038e-04,  6.4825e-04], device='cuda:0')
==> name fc2.weight torch.Size([2, 192]) tensor([[ 1.0039e-03, -1.1587e-03,  2.1375e-04,  5.3327e-04, -1.1424e-03,
         -8.9570e-04,  2.1679e-04, -7.3676e-04, -1.5854e-04,  1.2471e-03,
          7.1351e-04,  2.0531e-03, -1.7691e-03, -8.0522e-04, -2.3581e-03,
         -4.1970e-04,  1.5978e-04,  1.5900e-04,  3.3731e-04, -3.0590e-03,
          1.1869e-03,  9.1032e-05,  1.3800e-04,  1.1631e-03,  3.0448e-03,
          3.8991e-04,  3.6372e-04, -5.5915e-04, -2.2659e-03, -1.2969e-04,
         -1.0738e-03,  5.7826e-04,  1.5625e-03,  1.0711e-03, -5.7219e-04,
         -1.8515e-04,  1.1142e-03, -2.5288e-03,  4.6285e-04, -3.0486e-03,
          9.9918e-04,  1.5688e-03,  2.6713e-03, -1.8725e-03, -2.0208e-04,
         -4.1714e-04, -6.5882e-04, -5.1339e-04,  7.7247e-04, -1.2122e-03,
          1.0478e-03, -2.5368e-03,  8.2357e-04,  2.1989e-03,  2.7500e-03,
         -1.3418e-03, -1.5047e-03,  3.5788e-04, -2.2512e-03,  2.6732e-04,
         -1.8832e-04, -1.5624e-03, -1.1414e-03,  1.1938e-04,  1.9244e-03,
          1.8310e-03,  1.7244e-03, -1.3816e-03,  2.7161e-04, -1.5599e-03,
         -5.2565e-04,  2.5174e-04, -1.2621e-03, -6.3481e-04,  5.5840e-05,
         -1.7439e-03,  1.7155e-03, -6.1065e-04,  1.4176e-03,  1.4175e-03,
         -1.2566e-03, -1.5342e-03, -2.3960e-04,  1.8887e-03,  5.1703e-05,
         -6.1670e-05, -1.5579e-03,  1.3519e-04, -2.2427e-04, -1.1367e-03,
          1.1115e-03, -9.0533e-04,  8.0555e-05, -2.1167e-03, -1.3928e-03,
         -1.5621e-03,  1.5362e-03, -2.4000e-05,  1.0274e-04, -1.4278e-03,
         -4.6115e-04, -9.6383e-04,  2.0259e-03,  1.2325e-03, -4.7994e-04,
          7.4441e-04,  1.3236e-03,  3.1140e-05,  1.4101e-03,  1.3355e-03,
         -8.5969e-04,  4.4883e-04,  1.3180e-03, -5.6390e-04, -7.7154e-04,
         -7.9804e-04,  2.0541e-03,  5.3026e-04,  8.7902e-04, -1.1700e-03,
         -2.0984e-03, -5.1745e-05, -8.5713e-04, -1.4185e-03,  1.0406e-03,
         -6.6553e-04, -1.5462e-04,  5.5426e-04,  1.0779e-03,  9.7574e-04,
         -2.3676e-03, -8.5905e-04, -1.9207e-03,  4.1776e-04,  9.3451e-04,
         -1.5072e-03,  6.5093e-04,  2.2854e-03,  8.9238e-04,  3.3579e-04,
          3.8993e-04,  1.9168e-04,  4.9628e-04, -2.5741e-03, -1.1073e-03,
          6.8720e-04,  3.1319e-04,  2.5522e-03,  1.4039e-03, -6.3011e-04,
         -1.8413e-03,  1.0470e-03, -6.8893e-04, -3.2653e-03,  5.8914e-04,
         -2.6307e-05, -5.9981e-04,  3.0750e-03, -9.8649e-04, -1.1271e-03,
         -1.7952e-03,  3.0555e-04,  2.6237e-04, -2.5670e-03, -3.7551e-03,
          4.6197e-04,  1.6405e-03, -1.3298e-03,  3.3153e-05, -1.0062e-03,
         -1.1370e-03, -1.7913e-03,  2.4056e-05,  8.1597e-04,  2.0057e-04,
         -2.9867e-03, -8.0246e-04, -2.6573e-03, -1.4371e-03, -9.2468e-04,
         -1.6867e-03, -2.8074e-04, -1.8897e-03,  1.6246e-03,  2.8055e-03,
          1.3945e-03, -9.3832e-04,  6.2299e-04,  1.0941e-03,  2.0902e-03,
         -1.3390e-03,  1.1770e-03],
        [-9.4390e-05, -1.0729e-03, -2.5893e-04, -1.5264e-03,  9.2915e-05,
          1.0815e-03,  8.5908e-04,  2.2908e-03,  3.5516e-04,  7.2370e-04,
          5.1931e-04, -4.7434e-04,  1.3728e-03,  2.0921e-04, -1.4033e-03,
          4.6644e-05, -5.2135e-04, -9.0661e-04,  9.5042e-04,  1.2416e-03,
          5.8894e-04, -1.2142e-03,  8.8701e-04, -7.7844e-05, -3.8826e-04,
          2.9341e-04,  4.8494e-04, -8.0077e-04,  7.4322e-04, -2.0857e-04,
         -1.4400e-03,  2.1109e-03, -4.0077e-05,  5.6758e-04,  1.6402e-03,
          6.8955e-04, -1.8814e-03,  2.9283e-03,  7.8771e-04, -2.1951e-03,
          7.8559e-04, -1.2538e-04, -2.4112e-03, -2.0144e-04, -3.4403e-04,
          6.7416e-04,  8.2940e-05,  1.8444e-03, -3.3140e-04,  1.6746e-03,
         -2.4023e-03,  2.1146e-03, -1.1180e-03, -9.1027e-04, -1.3843e-03,
         -2.3300e-03, -1.8067e-03, -1.8440e-03,  8.9393e-04,  3.0532e-04,
         -7.0533e-04, -1.1048e-03,  2.4095e-03, -1.1106e-03, -3.7585e-03,
         -7.2092e-04, -8.0741e-04, -4.1021e-04, -6.2247e-04,  5.8192e-04,
         -1.3042e-03,  1.7833e-04, -3.5146e-04,  1.8592e-03, -5.4004e-04,
          7.3356e-05, -2.3105e-03,  8.8452e-04, -1.2621e-04, -1.9066e-04,
          2.2402e-03,  1.1415e-04, -4.9603e-05, -4.1960e-04, -2.3870e-03,
          8.1347e-04,  1.6523e-03, -1.1363e-03,  3.5938e-04, -1.9091e-03,
          2.2927e-04, -1.1081e-03, -8.1000e-04,  2.1634e-04,  3.5865e-04,
          1.0452e-03,  4.2076e-03, -1.3475e-03, -3.8908e-04,  2.7464e-04,
          2.1039e-03, -6.1510e-04,  1.0113e-03,  3.1765e-03,  2.0373e-04,
          5.9547e-05,  2.8756e-03, -5.4675e-04, -1.3340e-03,  1.7906e-03,
          1.2997e-03, -1.0709e-03, -1.3941e-04,  1.1134e-04,  2.7620e-04,
          4.7246e-04,  2.3401e-03,  1.7433e-03, -7.3268e-04,  4.9790e-04,
          1.2144e-03, -3.4891e-04,  1.1632e-03,  1.3969e-04,  3.0222e-04,
          5.6138e-04, -4.6011e-04, -1.1831e-04, -2.5964e-03,  1.3481e-03,
         -8.6936e-04, -1.4182e-04,  4.3698e-03,  1.0883e-03, -1.0411e-03,
          1.1948e-03,  8.3472e-05, -2.5857e-03,  1.0584e-03, -5.0073e-05,
         -9.8737e-05,  2.9661e-03,  9.0381e-04,  1.1945e-03,  2.3590e-03,
          2.0212e-03, -5.8487e-04, -1.0015e-03, -1.6407e-03,  1.2251e-03,
         -6.2431e-04,  1.8042e-03, -1.1849e-03,  1.0990e-03, -7.2428e-04,
          7.7698e-04, -1.2421e-03,  1.0248e-03, -8.6830e-04, -4.3260e-04,
          2.1995e-04, -8.9426e-05, -1.2943e-03,  3.4992e-05,  4.3080e-06,
         -7.5094e-04, -8.2188e-04, -1.4912e-03,  2.3899e-03, -6.3062e-04,
         -1.0462e-03,  1.4223e-03, -2.9927e-04, -6.2106e-04, -5.8662e-05,
         -4.3419e-04, -2.7051e-03,  4.5137e-04, -4.5899e-04,  1.7794e-04,
          4.5334e-04,  1.3250e-03, -8.8761e-04,  8.9815e-04, -8.1915e-04,
          7.1331e-04,  1.2659e-03, -2.6379e-04, -3.4659e-04,  8.9254e-04,
         -1.1084e-05, -1.0515e-03]], device='cuda:0')
==> name fc2.bias torch.Size([2]) tensor([-0.0058,  0.0058], device='cuda:0')
Iter:    150,  Train Loss:  0.24,  Train Acc: 48.44%,  Val Loss:  0.22,  Val Acc: 49.31%,  Time: 0:00:09 *,  LR: 0.002739052315863355
        [1.5271e-02, 9.8473e-01],
Traceback (most recent call last):
  File "/home/huyiwen/CV/bilstm/distill.py", line 71, in <module>
    student_train(T_model, S_model, cfg, train_loader, test_loader)
  File "/home/huyiwen/CV/bilstm/student.py", line 128, in student_train
    loss.backward()
  File "/home/huyiwen/miniconda3/envs/kd/lib/python3.10/site-packages/torch/_tensor.py", line 492, in backward
    torch.autograd.backward(
  File "/home/huyiwen/miniconda3/envs/kd/lib/python3.10/site-packages/torch/autograd/__init__.py", line 251, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt