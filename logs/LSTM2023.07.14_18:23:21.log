Config(class_list=<map object at 0x7f97d9fcde10>,
       train_path='data/train.json',
       test_path='data/test.json',
       teacher_save_path='saved_dict/teacher.ckpt',
       student_save_path='saved_dict/student.ckpt',
       device=device(type='cuda'),
       train_teacher=0,
       train_student=1,
       require_improvement=1000,
       num_classes=5,
       teacher_num_epochs=3,
       student_num_epochs=3,
       batch_size=64,
       pad_size=32,
       learning_rate=0.0005,
       bert_path='./bert_pretrain',
       tokenizer=PreTrainedTokenizer(name_or_path='./bert_pretrain', vocab_size=21128, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}),
       hidden_size=768,
       LSTM_bias=True,
       LSTM_peephole=False,
       use_mpo=True,
       mpo_type=['lstm'],
       truncate_num=10000,
       embedding_input_shape=<class 'list'>,
       embedding_output_shape=<class 'list'>,
       fc1_input_shape=(),
       fc1_output_shape=(),
       fc2_input_shape=(),
       fc2_output_shape=(),
       xh_input_shape=[30, 1, 1, 10],
       xh_output_shape=[30, 1, 1, 40],
       hh_input_shape=[30, 1, 1, 10],
       hh_output_shape=[30, 1, 1, 40])
加载数据...
Time usage: 0:00:03
Some weights of the model checkpoint at ./bert_pretrain were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/home/huyiwen/KnowledgeDistillation/distill.py", line 46, in <module>
    student_train(T_model, S_model, cfg, train_loader, test_loader)
  File "/home/huyiwen/KnowledgeDistillation/student.py", line 39, in student_train
    t_train_logits = teacher_predict(T_model, config, train_loader)
  File "/home/huyiwen/KnowledgeDistillation/teacher.py", line 27, in teacher_predict
    outputs = model(ids, mask)
  File "/home/huyiwen/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/huyiwen/KnowledgeDistillation/models/bert.py", line 18, in forward
    outputs = self.bert(context, attention_mask=mask)
  File "/home/huyiwen/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/huyiwen/anaconda3/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py", line 1014, in forward
    encoder_outputs = self.encoder(
  File "/home/huyiwen/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/huyiwen/anaconda3/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py", line 603, in forward
    layer_outputs = layer_module(
  File "/home/huyiwen/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/huyiwen/anaconda3/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py", line 531, in forward
    layer_output = apply_chunking_to_forward(
  File "/home/huyiwen/anaconda3/lib/python3.10/site-packages/transformers/pytorch_utils.py", line 246, in apply_chunking_to_forward
    return forward_fn(*input_tensors)
  File "/home/huyiwen/anaconda3/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py", line 544, in feed_forward_chunk
    layer_output = self.output(intermediate_output, attention_output)
  File "/home/huyiwen/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/huyiwen/anaconda3/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py", line 458, in forward
    hidden_states = self.LayerNorm(hidden_states + input_tensor)
KeyboardInterrupt
