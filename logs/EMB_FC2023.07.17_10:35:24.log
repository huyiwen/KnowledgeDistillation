Config(class_list=<map object at 0x7f17036e9e40>,
       train_path='data/train.json',
       test_path='data/test.json',
       teacher_save_path='saved_dict/teacher.ckpt',
       student_save_path='saved_dict/student.ckpt',
       device=device(type='cuda'),
       train_teacher=0,
       train_student=1,
       require_improvement=1000,
       num_classes=5,
       teacher_num_epochs=3,
       student_num_epochs=3,
       batch_size=64,
       pad_size=32,
       learning_rate=0.0005,
       bert_path='./bert_pretrain',
       tokenizer=PreTrainedTokenizer(name_or_path='./bert_pretrain', vocab_size=21128, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}),
       hidden_size=768,
       LSTM_bias=True,
       LSTM_peephole=False,
       use_mpo=True,
       mpo_type=['embedding', 'fc'],
       truncate_num=10000,
       embedding_input_shape=[19, 4, 2, 7, 20],
       embedding_output_shape=[10, 3, 1, 1, 10],
       fc1_input_shape=[10, 2, 1, 3, 10],
       fc1_output_shape=[6, 2, 1, 2, 8],
       fc2_input_shape=[6, 2, 1, 2, 8],
       fc2_output_shape=[5, 1, 1, 1, 1],
       xh_input_shape=(),
       xh_output_shape=(),
       hh_input_shape=(),
       hh_output_shape=())
加载数据...
Time usage: 0:00:03
Some weights of the model checkpoint at ./bert_pretrain were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
15,306,333 total parameters.
Epoch [1/3]
Iter:      0,  Train Loss: 1.1e+01,  Train Acc: 21.88%,  Val Loss: 1e+01,  Val Acc: 21.31%,  Time: 0:00:01 *
Iter:     50,  Train Loss:   5.1,  Train Acc: 57.81%,  Val Loss:   5.4,  Val Acc: 61.52%,  Time: 0:00:01 *
Iter:    100,  Train Loss:   4.0,  Train Acc: 60.94%,  Val Loss:   4.6,  Val Acc: 65.85%,  Time: 0:00:02 *
Iter:    150,  Train Loss:   3.8,  Train Acc: 71.88%,  Val Loss:   4.2,  Val Acc: 67.07%,  Time: 0:00:03 *
Iter:    200,  Train Loss:   4.3,  Train Acc: 67.19%,  Val Loss:   3.9,  Val Acc: 68.82%,  Time: 0:00:04 *
Epoch [2/3]
Iter:    250,  Train Loss:   2.5,  Train Acc: 82.81%,  Val Loss:   3.4,  Val Acc: 70.77%,  Time: 0:00:05 *
Iter:    300,  Train Loss:   2.5,  Train Acc: 81.25%,  Val Loss:   3.3,  Val Acc: 70.91%,  Time: 0:00:06 *
Iter:    350,  Train Loss:   2.5,  Train Acc: 76.56%,  Val Loss:   3.1,  Val Acc: 71.35%,  Time: 0:00:07 *
Iter:    400,  Train Loss:   2.8,  Train Acc: 79.69%,  Val Loss:   3.1,  Val Acc: 71.55%,  Time: 0:00:08 *
Iter:    450,  Train Loss:   2.1,  Train Acc: 73.44%,  Val Loss:   2.9,  Val Acc: 73.02%,  Time: 0:00:09 *
Epoch [3/3]
Iter:    500,  Train Loss:   2.2,  Train Acc: 81.25%,  Val Loss:   2.9,  Val Acc: 73.38%,  Time: 0:00:10 *
Iter:    550,  Train Loss:   2.0,  Train Acc: 78.12%,  Val Loss:   2.8,  Val Acc: 73.35%,  Time: 0:00:11 *
Iter:    600,  Train Loss:   1.8,  Train Acc: 81.25%,  Val Loss:   2.8,  Val Acc: 74.10%,  Time: 0:00:12 *
Iter:    650,  Train Loss:   1.4,  Train Acc: 89.06%,  Val Loss:   2.8,  Val Acc: 73.58%,  Time: 0:00:12 
Iter:    700,  Train Loss:   1.5,  Train Acc: 79.69%,  Val Loss:   2.8,  Val Acc: 73.71%,  Time: 0:00:13 
